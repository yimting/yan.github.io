<!DOCTYPE html>












  


<html class="theme-next muse use-motion" lang="zh-CN">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#B0E0E6">
























<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2">

<link rel="stylesheet" href="/css/main.css?v=7.1.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/Head.jpg?v=7.1.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/Head.jpg?v=7.1.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/logo.svg?v=7.1.0">


  <link rel="mask-icon" href="/images/logo.svg?v=7.1.0" color="#B0E0E6">







<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '7.1.0',
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false,"dimmer":false},
    back2top: true,
    back2top_sidebar: false,
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="一面（2019.04.29）： 怎么进行数仓数据管理？ 参考《大数据之路：阿里巴巴大数据实践》，可从以下四个方面进行考虑： 1.1 元数据管理 元数据（Meta Date），其实应该叫做解释性数据，或者数据字典，即数据的数据。主要记录数据仓库中模型的定义、各层级间的映射关系、监控数据仓库的数据状态及ETL的任务运行状态。一般会通过 元数据资料库（Metadata Repository）来统一地存储">
<meta name="keywords" content="面试">
<meta property="og:type" content="article">
<meta property="og:title" content="WL面试">
<meta property="og:url" content="http://yimting.github.io/2019/06/26/WL面试/index.html">
<meta property="og:site_name" content="Baki&#39;Blog">
<meta property="og:description" content="一面（2019.04.29）： 怎么进行数仓数据管理？ 参考《大数据之路：阿里巴巴大数据实践》，可从以下四个方面进行考虑： 1.1 元数据管理 元数据（Meta Date），其实应该叫做解释性数据，或者数据字典，即数据的数据。主要记录数据仓库中模型的定义、各层级间的映射关系、监控数据仓库的数据状态及ETL的任务运行状态。一般会通过 元数据资料库（Metadata Repository）来统一地存储">
<meta property="og:locale" content="zh-CN">
<meta property="og:updated_time" content="2019-06-29T03:18:22.231Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="WL面试">
<meta name="twitter:description" content="一面（2019.04.29）： 怎么进行数仓数据管理？ 参考《大数据之路：阿里巴巴大数据实践》，可从以下四个方面进行考虑： 1.1 元数据管理 元数据（Meta Date），其实应该叫做解释性数据，或者数据字典，即数据的数据。主要记录数据仓库中模型的定义、各层级间的映射关系、监控数据仓库的数据状态及ETL的任务运行状态。一般会通过 元数据资料库（Metadata Repository）来统一地存储">





  
  
  <link rel="canonical" href="http://yimting.github.io/2019/06/26/WL面试/">



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>WL面试 | Baki'Blog</title>
  












  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Baki'Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
      
        <p class="site-subtitle">yimting@aliyun.com</p>
      
    
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">

    
    
    
      
    

    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>首页</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-about">

    
    
    
      
    

    

    <a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i> <br>关于</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">

    
    
    
      
    

    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br>标签</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">

    
    
    
      
    

    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>归档</a>

  </li>

      
      
    </ul>
  

  
    

  

  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yimting.github.io/2019/06/26/WL面试/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="yimting@aliyun.com">
      <meta itemprop="description" content="路漫漫其修远兮">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Baki'Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">WL面试

              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-06-26 12:38:20" itemprop="dateCreated datePublished" datetime="2019-06-26T12:38:20+08:00">2019-06-26</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-06-29 11:18:22" itemprop="dateModified" datetime="2019-06-29T11:18:22+08:00">2019-06-29</time>
              
            
          </span>

          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h3 id="一面（2019-04-29）："><a href="#一面（2019-04-29）：" class="headerlink" title="一面（2019.04.29）："></a>一面（2019.04.29）：</h3><ol>
<li><p><strong>怎么进行数仓数据管理？</strong></p>
<p>参考《大数据之路：阿里巴巴大数据实践》，可从以下四个方面进行考虑：</p>
<p>1.1 <strong>元数据管理</strong></p>
<p>元数据（Meta Date），其实应该叫做解释性数据，或者数据字典，即数据的数据。主要记录数据仓库中模型的定义、各层级间的映射关系、监控数据仓库的数据状态及ETL的任务运行状态。<!--readmore-->一般会通过 <strong>元数据资料库</strong>（Metadata Repository）来统一地存储和管理元数据，其主要目的是使数据仓库的设计、部署、操作和管理能达成协同和一致。</p>
<p>最后做个Ending，数据仓库本身既不生产数据也不消费数据，只是作为一个中间平台集成化地存储数据；数据仓库实现的难度在于整体架构的构建及ETL的设计，这也是日常管理维护中的重头；而数据仓库的真正价值体现在于基于其的数据应用上，如果没有有效的数据应用也就失去了构建数据仓库的意义。</p>
<p>1.2 <strong>计算管理</strong></p>
<p>主要包括系统优化和任务优化</p>
<p>系统优化，主要是基于Hadoop等分布式计算系统评估资源的方式，一般是根据Map端输入数据量进行静态评估，在任务稳定的情况下，可以考虑基于任务的历史执行情况进行资源评估，即采用HBO（History-Based Optimizer 基于历史的优化器）。另外一种CBO（Cost-Based Optimizer，基于代价的优化器）</p>
<p>任务优化，主要从数据倾斜方面，例如Map数据倾斜解决方案：通过对上游合并小文件，同时调节本节点的小文件参数设置：“ set odps.sql.  mapper.merge.limit.size 64 ”和“ set odps .s ql.mapper.s plit.size=256 个参数来调节（Map Instance个数和Map Instance读取小文件个数）</p>
<p>1.3 <strong>存储和成本管理</strong></p>
<p>主要从数据压缩，数据重分布，存储治理项优化，生命周期管理等的角度。</p>
<p>例如数据重分布：在MaxCompute （大数据质量计算）中主要采用基于列式存储的方式，由于每个表的数据 分布不同，插人数据的顺序不 样，会导致压缩效果有很大的差异， 此通过修改表的数据重分布，避免列热点，将会节省一定的存储空间。目前我们主要通过修改 distribute by sort by 字段的方法进行数据重分布。</p>
<p>1.4 <strong>数据质量</strong></p>
<p>数据质量保障原则，对数据库的完整性，准确性，一致性，及时性进行评估</p>
</li>
</ol>
<ol start="2">
<li><p><strong>什么是数仓血缘关系？</strong> </p>
<p>一种构建数据仓库表血缘关系图的方法，其特征在于，所述方法包括: 解析访问数据仓库的每个<strong>数据仓库操作语句</strong>，得到所述每个数据仓库操作语句访问的数据仓库<strong>目的表的表名</strong>； 将所述每个数据仓库操作语句的<strong>语句标识</strong>与访问的数据仓库目的表的表名的对应关系存储在对应关系表中； 根据所述对应关系表，获取所述对应关系表中的每个数据仓库目的表对应的<strong>数据仓库来源表的表名</strong>； 根据每个数据仓库目的表的表名和所述每个数据仓库目的表对应的数据仓库来源表的表名，构建数据仓库表<strong>血缘关系图</strong>。 </p>
<p>主要作用：有了血缘关系，基于开源jsmind库做了展示，<strong>可以让用户清楚看到一张表的上下游，更方便地查找表</strong>。基于血缘关系可以做很多事情，例如：</p>
<ul>
<li><p>结合表的更新时间，还可以找到调度DAG的关键路径，协助定位性能瓶颈；</p>
</li>
<li><p>当表出现变更时，可以通知下游责任人，以及自动对下游任务做SQL的静态检查；</p>
</li>
<li><p>辅助生命周期管理，找到没有被使用的表/字段；</p>
</li>
<li><p>辅助维护字段的一致性，如注释、校验规则复用。</p>
</li>
</ul>
</li>
</ol>
<ol start="3">
<li><p><strong>数仓的数据同步问题怎么解决？</strong> </p>
<p><a href="https://www.jiqizhixin.com/articles/2018-12-07-19" target="_blank" rel="noopener">https://www.jiqizhixin.com/articles/2018-12-07-19</a> </p>
<p>如何准确、高效地把MySQL数据同步到Hive中？一般常用的解决方案是批量取数并Load：直连MySQL去Select表中的数据，然后存到本地文件作为中间存储，最后把文件Load到Hive表中。 但这种方法容易造成慢查询以及Hive的性能瓶颈</p>
<p>为了彻底解决这些问题，我们逐步转向CDC (Change Data Capture) + Merge的技术方案， 实时Binlog采集 + 离线处理Binlog还原业务数据这样一套解决方案。Binlog是MySQL的二进制日志，记录了MySQL中发生的所有数据变更，MySQL集群自身的主从同步就是基于Binlog做的。</p>
<p>具体步骤分为以下两步：</p>
<blockquote>
<ol>
<li><p>Binlog实时采集：</p>
<p>在Binlog实时采集方面，我们采用了阿里巴巴的开源项目Canal，负责从MySQL实时拉取Binlog并完成适当解析。Binlog采集后会暂存到Kafka上供下游消费 。对Binlog的实时采集包含两个主要模块：一是CanalManager，主要负责采集任务的分配、监控报警、元数据管理以及和外部依赖系统的对接；二是真正执行采集任务的Canal和CanalClient。 </p>
<ul>
<li><p>开启mysql的bin-log</p>
<pre><code>开启方式，修改my.cnf(/etc/my.cnf) 添加如下内容：
server-id=1
log-bin=master（这一步开启binlog）
binlog_format=row（这里format必须是row模式）
</code></pre></li>
<li><p>重启mysql服务service mysqld restart</p>
</li>
<li><p>安装部署canal server，修改配置并启动</p>
<pre><code>wget https://github.com/alibaba/canal/releases/download/canal-1.0.24/canal.deployer-1.0.24.tar.gz
mkdir canal-1.0.24
tar -zxvf canal.deployer-1.0.24.tar.gz -C canal-1.0.24
vim canal-1.0.24/conf/example/instance.properties：配置监控指定数据库的
canal-1.0.24/bin/startup.sh
</code></pre></li>
<li><p>编写canal客户端canal server</p>
</li>
</ul>
<p>原文：<a href="https://blog.csdn.net/u011563666/article/details/81331971" target="_blank" rel="noopener">https://blog.csdn.net/u011563666/article/details/81331971</a> </p>
</li>
<li><p>离线还原Mysql数据：</p>
<ol>
<li>采用Linkedin的开源项目Camus，负责每小时把Kafka上的Binlog数据拉取到Hive上。</li>
<li>对每张ODS表，首先需要一次性制作快照（Snapshot），把MySQL里的存量数据读取到Hive上，这一过程底层采用直连MySQL去Select数据的方式。</li>
<li>对每张ODS表，每天基于存量数据和当天增量产生的Binlog做Merge，从而还原出业务数据。  </li>
</ol>
</li>
</ol>
</blockquote>
<p>我们回过头来看看，背景中介绍的批量取数并Load方案遇到的各种问题，为什么用这种方案能解决上面的问题呢？</p>
<ul>
<li>首先，Binlog是流式产生的，通过对Binlog的实时采集，把部分数据处理需求由每天一次的批处理分摊到实时流上。无论从性能上还是对MySQL的访问压力上，都会有明显地改善。</li>
<li>第二，Binlog本身记录了数据变更的类型（Insert/Update/Delete），通过一些语义方面的处理，完全能够做到精准的数据还原。</li>
</ul>
<p><strong>慢查询</strong> ：<a href="https://zhuanlan.zhihu.com/p/25648377" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/25648377</a> </p>
<p>作用：可通过开启慢查询定位到影响性能瓶颈的SQL所在位置，传统的方法可能是在SQL执行前后打上时间戳来进行估计查询时间，暂且不论由于各种因素的影响这种估算可能不准确，更让人不可接受的是这对原始代码造成的极大的侵入 。好在 MySQL 提供了慢查询日志。这个日志会记录所有执行时间超过 long_query_time（默认是 10s）的 SQL 及相关的信息。 </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; show variables like <span class="string">'long_query_time'</span>;</span><br><span class="line">+-----------------+-----------+</span><br><span class="line">| Variable_name   | Value     |</span><br><span class="line">+-----------------+-----------+</span><br><span class="line">| long_query_time | <span class="number">10.000000</span> |<span class="comment">//定义SQL执行时间超过多少为慢查询</span></span><br><span class="line">+-----------------+-----------+</span><br><span class="line">mysql&gt; show variables like <span class="string">'slow_query%'</span>;</span><br><span class="line">+---------------------+--------------------------------------+</span><br><span class="line">| Variable_name       | Value                                |</span><br><span class="line">+---------------------+--------------------------------------+</span><br><span class="line">| slow_query_log      | OFF                                  |<span class="comment">//慢查询开关</span></span><br><span class="line">| slow_query_log_file | /<span class="keyword">var</span>/log/mysql/log-slow-queries.log  |<span class="comment">//发生慢查询SQL文件地址</span></span><br><span class="line">+---------------------+--------------------------------------+</span><br><span class="line"><span class="comment">//在开启了 MySQL 慢查询日志一段时间之后，日志中就会把所有超过 long_query_time 的 SQL 记录下来</span></span><br><span class="line"><span class="comment">//在得知哪些 SQL 是慢查询之后，我们就可以定位到具体的业务接口并针对性的进行优化了。</span></span><br></pre></td></tr></table></figure>
</li>
</ol>
<ol start="4">
<li><p><strong>为什么要进行stage的划分？</strong></p>
<p>原始的RDD通过一系列的转换形成DAG，根据RDD之间的依赖关系的不同将DAG划分成不同的Stage，对于窄依赖，partition的转换处理在Stage中完成计算，对于宽依赖，由于有shuffle的存在，只能在parent RDD处理完成后，才能开始接下来的计算，所以需要Stage的划分将数据RDD的处理在每个Stage处理完成后再往下进行。</p>
</li>
</ol>
<ol start="5">
<li><p><strong>为什么要创建ods层？如何优雅的分层？</strong></p>
<p><a href="https://www.kancloud.cn/grass1314521/data_warehouse_in_action/490402" target="_blank" rel="noopener">https://www.kancloud.cn/grass1314521/data_warehouse_in_action/490402</a> </p>
<p> 我们的数据主要会有两个大的来源：</p>
<ol>
<li>业务库，这里经常会使用 Sqoop 来抽取，比如我们每天定时抽取一次。在实时方面，可以考虑用 Canal 监听 Mysql 的 Binlog，实时接入即可。</li>
<li>埋点日志，线上系统会打入各种日志，这些日志一般以文件的形式保存，我们可以选择用 Flume 定时抽取，也可以用 Spark Streaming 或者 Storm 来实时接入，当然，Kafka 也会是一个关键的角色。</li>
<li>其它数据源会比较多样性，这和具体的业务相关，不再赘述。</li>
</ol>
<p>数据运营层，也叫ODS层，是最接近数据源中数据的一层，数据源中的数据，经过抽取、洗净、传输，也就说传说中的 ETL 之后，装入本层。本层的数据，总体上大多是按照源头业务系统的分类方式而分类的。</p>
<p>但是，这一层面的数据却不等同于原始数据。在源数据装入这一层时，要进行诸如<strong>去噪</strong>（例如有一条数据中人的年龄是 300 岁，这种属于异常数据，就需要提前做一些处理）、<strong>去重</strong>（例如在个人资料表中，同一 ID 却有两条重复数据，在接入的时候需要做一步去重）、字段<strong>命名规范</strong>等一系列操作。</p>
<p><strong>注意：</strong> 在这层，理应不是简单的数据接入，而是要考虑一定的数据清洗，比如异常字段的处理、字段命名规范化、时间字段的统一等，一般这些很容易会被忽略，但是却至关重要。<strong>特别是后期我们做各种特征自动生成的时候，会十分有用。</strong> </p>
</li>
</ol>
<h3 id="老陈二面（2019-05-06）："><a href="#老陈二面（2019-05-06）：" class="headerlink" title="老陈二面（2019.05.06）："></a>老陈二面（2019.05.06）：</h3><ol>
<li><p>自我介绍</p>
</li>
<li><p>为什么离职</p>
</li>
<li><p>你觉得做的比较好的一个项目？项目描述？（可细致一些，控制时长到半小时左右最好）项目架构？</p>
</li>
<li><p>ETL规则</p>
<blockquote>
<p>清洗规则：数据清洗的任务是过滤那些不符合要求的数据，将过滤的结果交给业务主管部门，确认是否过滤掉还是由业务单位修正之后再进行抽取。</p>
<p><strong>不符合要求的数据主要是有不完整的数据、错误的数据、重复的数据三大类。</strong></p>
<p>　　<strong>(1)不完整的数据：</strong>这一类数据主要是一些应该有的信息缺失，如供应商的名称、分公司的名称、客户的区域信息缺失、业务系统中主表与明细表不能匹配等。对于这一类数据过滤出来，按缺失的内容分别写入不同Excel文件向客户提交，要求在规定的时间内补全。补全后才写入数据仓库。</p>
<p>　　<strong>(2)错误的数据：</strong>这一类错误产生的原因是业务系统不够健全，在接收输入后没有进行判断直接写入后台数据库造成的，比如数值数据输成全角数字字符、字符串数据后面有一个回车操作、日期格式不正确、日期越界等。这一类数据也要分类，对于类似于全角字符、数据前后有不可见字符的问题，<strong>只能通过写SQL语句的方式找出来，然后要求客户在业务系统修正之后抽取</strong>。日期格式不正确的或者是日期越界的这一类<strong>错误会导致ETL运行失败，</strong>这一类错误需要去业务系统数据库用SQL的方式挑出来，交给业务主管部门要求限期修正，修正之后再抽取。</p>
<p>　　<strong>(3)重复的数据：</strong>对于这一类数据——特别是维表中会出现这种情况——将重复数据记录的所有字段导出来，让客户确认并整理。</p>
<p>　　数据清洗是一个反复的过程，不可能在几天内完成，只有不断的发现问题，解决问题。对于是否过滤，是否修正一般要求客户确认，对于过滤掉的数据，写入Excel文件或者将过滤数据写入数据表，在ETL开发的初期可以每天向业务单位发送过滤数据的邮件，促使他们尽快地修正错误,同时也可以做为将来验证数据的依据。数据清洗需要注意的是不要将有用的数据过滤掉，对于每个过滤规则认真进行验证，并要用户确认。</p>
</blockquote>
</li>
<li><p>dw层建模步骤</p>
<p>首先数仓的特性：是一个面向主题的、集成的、非易失的和时变的数据集合，用于支持管理者的决策过程</p>
<p>维度模型通常被一种称为星型模型的方式构建，步骤如下：（以下四步都是层级关系，下一步依赖于上一步，需要保持一致）</p>
<p><strong>选择业务流程</strong>（主题）：例如，需要了解和分析一个零售店的销售情况，那么与该零售店销售相关的所有业务流程都是需要关注的</p>
<p><strong>声明粒度</strong>    ：例如，一个零售店的顾客在购物小票上的一个购买条目</p>
<p><strong>确认维度</strong>：例如，日期维度应该包括年、季度、月、周、日等数据。典型的维度都是名词</p>
<p><strong>确认事实</strong>：大部分事实表的度量都是数字类型的，可累加，可计算，如成本、数量、金额等。</p>
</li>
<li><p>多少job（指标）</p>
<p>本项目一共周期为27周，ETL用了3个月，剩余四个月，每个人平均一天能处理8-12个指标，</p>
<p>按照三个人来进行计算那么一共需要处理指标数：3x8x22x4=2112，可以回答一共1500多个指标</p>
</li>
</ol>
<h3 id="学长分享（2019-05-19）："><a href="#学长分享（2019-05-19）：" class="headerlink" title="学长分享（2019.05.19）："></a>学长分享（2019.05.19）：</h3><ol>
<li><p>grouping sets与group by</p>
<p>grouping sets就是由多个group by联合起来,如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">select A , <span class="function">B from table group by grouping <span class="title">sets</span><span class="params">(A, B)</span>   等价于</span></span><br><span class="line"><span class="function">select A , <span class="keyword">null</span> as B  from table group by A  </span></span><br><span class="line"><span class="function">union all  </span></span><br><span class="line"><span class="function">select <span class="keyword">null</span> as A ,  B  from table group by B</span></span><br></pre></td></tr></table></figure>
<p>上面提到grouping sets是等价于带union all的group by子句，之所以是等价而不是等于，从两者结果集中的对比就可以一目了之，那就是它们的顺序不一样。这说明grouping sets并不只是group by的语法糖，这两者内部的执行过程应该是全然不同的，在百度过程中发现大多数答案都是这句话：“聚合是一次性从数据库中取出所有需要操作的数据，在内存中对数据库进行聚合操作并生成结果。而UNION ALL是多次扫描表，将返回的结果进行UNION操作。性能方面grouping sets能减少IO操作但会增加CPU占用时间”。 </p>
</li>
</ol>
<h3 id="面试准备："><a href="#面试准备：" class="headerlink" title="面试准备："></a><strong>面试准备：</strong></h3><h5 id="JVM调优"><a href="#JVM调优" class="headerlink" title="JVM调优"></a>JVM调优</h5><ol>
<li><h5 id="什么是JVM？怎么理解JVM？JVM优化？"><a href="#什么是JVM？怎么理解JVM？JVM优化？" class="headerlink" title="什么是JVM？怎么理解JVM？JVM优化？"></a><strong>什么是JVM？怎么理解JVM？JVM优化？</strong></h5><p>JVM理解可以按照以下方式进行回答：E:\面试prepare\JVM解答</p>
<p>JVM调优：<a href="https://juejin.im/post/59f02f406fb9a0451869f01c#heading-2" target="_blank" rel="noopener">https://juejin.im/post/59f02f406fb9a0451869f01c#heading-2</a> </p>
<p>jvm调优主要是<strong>针对垃圾收集器的收集性能优化</strong>，令运行在虚拟机上的应用能够使用更少的内存以及延迟获取更大的吞吐量。当然这里的最少是最优的选择，而不是越少越好。 </p>
<blockquote>
<p>内存占用：垃圾收集器流畅运行所需要的内存数量</p>
<p>延迟：其度量标准是缩短由于垃圾收集引起的停顿时间或者完全消除因垃圾收集所引起的停顿，避免应用运行时发生抖动。</p>
<p>吞吐量：重要指标之一，是指不考虑垃圾收集引起的停顿时间或内存消耗，垃圾收集器能支撑应用达到的最高性能指标</p>
</blockquote>
<p>调优原则：</p>
<blockquote>
<ol>
<li>MinorGC回收原则： 每次minor GC 都要尽可能多的收集垃圾对象。以减少应用程序发生Full GC的频率。</li>
<li>GC内存最大化原则：处理吞吐量和延迟问题时候，垃圾处理器能使用的内存越大，垃圾收集的效果越好，应用程序也会越来越流畅。</li>
<li>GC调优3选2原则: 在性能属性里面，吞吐量、延迟、内存占用，我们只能选择其中两个进行调优，不可三者兼得。</li>
</ol>
<p>另外调优一般是从满足程序的内存使用需求开始的，之后是时间延迟的要求，最后才是吞吐量的要求，要基于这个步骤来不断优化，每一个步骤都是进行下一步的基础，不可逆行之</p>
</blockquote>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//堆内存查看命令</span></span><br><span class="line">jstat -class pid:显示加载class的数量，及所占空间等信息。 </span><br><span class="line">jstat -compiler pid:显示VM实时编译的数量等信息。 </span><br><span class="line">jstat -gc pid:可以显示gc的信息，查看gc的次数，及时间。其中最后五项，分别是young gc的次数，young gc的时间，full gc的次数，full gc的时间，gc的总时间。 </span><br><span class="line">jstat -gccapacity:可以显示，VM内存中三代（young,old,perm）对象的使用和占用大小，如：PGCMN显示的是最小perm的内存使用量，PGCMX显示的是perm的内存最大使用量，PGC是当前新生成的perm内存占用量，PC是但前perm内存占用量。其他的可以根据这个类推， OC是old内纯的占用量。</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>   <strong>内存调优</strong> ：</p>
<p>   需要产生足够的压力，找到应用程序和生产环境高峰符合状态类似的负荷，在此之后达到峰值之后，保持一个稳定的状态 ，从gc日志中分析，在发生fullGC之时，整个应用的堆占用以及GC时间，当然了，为了更加精确，应该多收集几次，获取一个平均值。或者是采用耗时最长的一次FullGC来进行估算。 然后根据FullGC之后的其他堆空间（老年代，永久代等）大小进行相对应堆空间大小参数的调整，例如：</p>
<p>   空间：java heap         命令参数：-Xms和-Xmx         建议扩大倍数：3-4倍FullGC后的老年代空间占用 </p>
<p>   <strong>延迟调优</strong>：</p>
<p>   应用程序可接受的平均停滞时间: 此时间与测量的Minor GC持续时间进行比较。</p>
<p>   可接受的Minor GC频率：Minor GC的频率与可容忍的值进行比较。</p>
<p>   可接受的最大停顿时间: 最大停顿时间与最差情况下FullGC的持续时间进行比较。</p>
<p>   可接受的最大停顿发生的频率：基本就是FullGC的频率。</p>
<p>   然后根据轻度回收和全回收的时间以及频次进行新生代和老年代大小的调整以达到用户能接受的范围</p>
<p>   <strong>吞吐量调优</strong> ：</p>
<p>   吞吐量调优主要是基于应用程序的吞吐量要求而来的，应用程序应该有一个综合的吞吐指标，这个指标基于真个应用的需求和测试而衍生出来的。当有应用程序的吞吐量达到或者超过预期的吞吐目标，整个调优过程就可以圆满结束了。</p>
<p>   如果出现调优后依然无法达到应用程序的吞吐目标，需要重新回顾吞吐要求，评估当前吞吐量和目标差距是否巨大，如果在20%左右，可以修改参数，加大内存，再次从头调试，如果巨大就需要从整个应用层面来考虑，设计以及目标是否一致了，重新评估吞吐目标。</p>
<h5 id="Hive任务优化以及调优"><a href="#Hive任务优化以及调优" class="headerlink" title="Hive任务优化以及调优"></a>Hive任务优化以及调优</h5><ol>
<li><h5 id="Hive任务优化"><a href="#Hive任务优化" class="headerlink" title="Hive任务优化"></a>Hive任务优化</h5><h5 id="基本生产环境中的调优都是对小文件以及数据倾斜的处理"><a href="#基本生产环境中的调优都是对小文件以及数据倾斜的处理" class="headerlink" title="基本生产环境中的调优都是对小文件以及数据倾斜的处理"></a><strong>基本生产环境中的调优都是对小文件以及数据倾斜的处理</strong></h5><p>E:\面试prepare\Hive任务优化–控制hive任务中的map数和reduce数</p>
<p>将小文件进行合并成大文件处理减少map数，可以设置以下参数：</p>
<blockquote>
<p>set mapred.max.split.size=100000000; </p>
<p>set mapred.min.split.size.per.node=100000000; </p>
<p>set mapred.min.split.size.per.rack=100000000; </p>
<p>set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat; 此参数表示执行前进行小文件的合并</p>
<p>前面三个参数确定合并文件块的大小，大于文件块大小128m的，按照128m来分隔， </p>
<p>小于128m,大于100m的，按照100m来分隔，把那些小于100m的（包括小文件和分隔大文 </p>
<p>件剩下的）</p>
</blockquote>
<p><strong>什么情况下只有一个reduce？</strong></p>
<blockquote>
<ol>
<li><p>数据量小于hive.exec.reducers.bytes.per.reducer 参数值</p>
</li>
<li><p>没有group by的汇总，比如把select pt,count(1) from popt_tbaccountcopy_mes </p>
<p>where pt = ‘2012-07-04’ group by pt; 写成 select count(1) from </p>
<p>popt_tbaccountcopy_mes where pt = ‘2012-07-04’; </p>
</li>
<li><p>用了Order by </p>
</li>
<li><p>有笛卡儿积</p>
</li>
</ol>
</blockquote>
<p><strong>hive调优</strong>:<a href="https://www.cnblogs.com/ITtangtang/p/7683028.html" target="_blank" rel="noopener">https://www.cnblogs.com/ITtangtang/p/7683028.html</a> </p>
<blockquote>
<ol>
<li><p>开启limit，防止过多的数据查询造成时延</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&gt;    一般情况下，Limit语句还是需要执行整个查询语句，然后再返回部分结果。</span><br><span class="line">&gt;    </span><br><span class="line">&gt;    有一个配置属性可以开启，避免这种情况---对数据源进行抽样</span><br><span class="line">&gt;    </span><br><span class="line">&gt;    hive.limit.optimize.enable=<span class="keyword">true</span> --- 开启对数据源进行采样的功能</span><br><span class="line">&gt;    </span><br><span class="line">&gt;    hive.limit.row.max.size --- 设置最小的采样容量</span><br><span class="line">&gt;    </span><br><span class="line">&gt;    hive.limit.optimize.limit.file --- 设置最大的采样样本数</span><br><span class="line">&gt;    </span><br><span class="line">&gt;    缺点：有可能部分数据永远不会被处理到</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>
</li>
</ol>
</blockquote>
<blockquote>
<ol start="2">
<li><p>开启严格模式禁止三种模式的查询</p>
<p>（查询全分区没有where限制的，使用order by 没有limit，表与表进行连接是没有使用on加以限制的）</p>
</li>
<li><p>合理的设置map reduce的个数</p>
</li>
<li><p>对于较小的数据使用本地模式</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&gt;    有时hive的输入数据量是非常小的。在这种情况下，为查询出发执行任务的时间消耗可能会比实际job的执行时间要多的多。对于大多数这种情况，hive可以通过本地模式在单台机器上处理所有的任务。对于小数据集，执行时间会明显被缩短</span><br><span class="line">&gt;    set hive.exec.mode.local.auto=<span class="keyword">true</span>;</span><br><span class="line">&gt;    当一个job满足如下条件才能真正使用本地模式：</span><br><span class="line">&gt;    　　<span class="number">1</span>.job的输入数据大小必须小于参数：hive.exec.mode.local.auto.inputbytes.max(默认<span class="number">128</span>MB)</span><br><span class="line">&gt;    　　<span class="number">2</span>.job的map数必须小于参数：hive.exec.mode.local.auto.tasks.max(默认<span class="number">4</span>)</span><br><span class="line">&gt;    　　<span class="number">3</span>.job的reduce数必须为<span class="number">0</span>或者<span class="number">1</span></span><br><span class="line">&gt;    可用参数hive.mapred.local.mem(默认<span class="number">0</span>)控制child jvm使用的最大内存数。</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>
</li>
</ol>
</blockquote>
<blockquote>
<ol start="5">
<li><p>设置并行度，让一些不相关的stage能够并行运行</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt;    hive会将一个查询转化为一个或多个阶段，包括：MapReduce阶段、抽样阶段、合并阶段、limit阶段等。默认情况下，一次只执行一个阶段。 不过，如果某些阶段不是互相依赖，是可以并行执行的。</span><br><span class="line">&gt;    set hive.exec.parallel=<span class="keyword">true</span>,可以开启并发执行。</span><br><span class="line">&gt;    set hive.exec.parallel.thread.number=<span class="number">16</span>; <span class="comment">//同一个sql允许最大并行度，默认为8。</span></span><br><span class="line">&gt;    会比较耗系统资源。</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>
</li>
</ol>
</blockquote>
<blockquote>
<ol start="6">
<li><p>Jvm的重用 通过配置文件的从那时候，来配置某些task的stage使用同一个jvm，不必反复开关造成资源的浪费</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt;    <span class="comment">//用于避免小文件的场景或者task特别多的场景，这类场景大多数执行时间都很短，因为hive调起mapreduce任务，JVM的启动过程会造成很大的开销，尤其是job有成千上万个task任务时，JVM重用可以使得JVM实例在同一个job中重新使用N次</span></span><br><span class="line">&gt;    set mapred.job.reuse.jvm.num.tasks=<span class="number">10</span>; --<span class="number">10</span>为重用个数</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>
</li>
</ol>
</blockquote>
<blockquote>
<ol start="7">
<li><p>建立索引，提升查询的效率</p>
</li>
<li><p>对传输的文件进行压缩，减少空间的占用</p>
</li>
</ol>
</blockquote>
</li>
</ol>
<h5 id="hive数据倾斜"><a href="#hive数据倾斜" class="headerlink" title="hive数据倾斜"></a><strong>hive数据倾斜</strong></h5><blockquote>
<p>join导致数据倾斜问题排查步骤：<a href="https://blog.csdn.net/wisgood/article/details/77063606" target="_blank" rel="noopener">https://blog.csdn.net/wisgood/article/details/77063606</a> </p>
<ol>
<li><p>如何判断是大Key导致的问题：</p>
<ol>
<li>通过时间判断如果某个reduce的时间比其他reduce时间长的多。（注意：如果每个reduce执行时间差不多，都特别长，则可能是reduce设置过少导致的 ）</li>
<li>通过任务Counter判断，Counter会记录整个job以及每个task的统计信息。通过输入记录数或者输出字符数对比</li>
</ol>
</li>
<li><p>如何找到大Key及对应SQL执行代码</p>
<p>找到对应大Key，一般情况下，hive在做join的时候，会打印join的日志，我们通过日志查找大Key。</p>
<p>首先找到任务特别慢的那个task，打开对应的日志，然后搜索日志中出现的“rows for joinkey”，找到时间跨度最长的那条记录</p>
</li>
<li><p>确定任务卡住的stage</p>
<ol>
<li>通过jobname确定stage，一般通过Hive默认jobname会带上stage阶段名称，例如：Stage-1</li>
<li>如果jobname是自定义的，那可能没法通过jobname判断stage。需要借助于任务日志。找到执行特别慢的那个task，搜索 “CommonJoinOperator: JOIN struct” 。Hive在做join的时候，会把join的key打印到日志中 </li>
<li>这时候需要参考改SQL的执行计划，通过参考执行计划可以确定该阶段stage</li>
</ol>
</li>
<li><p>确定SQL执行代码</p>
<p>确定了执行阶段（即stage），那么通过执行计划，就可以判断出是执行哪段代码时出现了倾斜。 </p>
</li>
</ol>
<p>关于大Key数据倾斜的处理方案：</p>
<ol>
<li>过滤掉脏数据：如果大Key是毫无意义的脏数据，直接过滤掉</li>
<li>数据预处理，尽量保证join的时候，同一个key对应的记录不要太多</li>
</ol>
<p>导致数据倾斜原因：</p>
<ol>
<li>key分布不均匀</li>
<li>业务数据本身问题</li>
<li>某些SQL语句本身存在问题</li>
<li>建表时考虑不周</li>
</ol>
<p>解决方案（本质上就是对于发生数据倾斜的key进行预处理）：</p>
<ol>
<li><p>参数调节：</p>
<p>Hive.map.aggr=true    设置在map端进行combiner，提前进行计算任务，在map端就减小数据量，然后减轻shuffle到reduce的io</p>
<p>Hive.groupby.skewindata =true    数据倾斜的时候进行负载均衡，产生两个MRjob，一个的Map 的输出结果集合会随机分布到 Reduce 中， 另一个MR Job 再根据预处理的数据结果按照 Group By Key 分布到 Reduce 中（这个过程可以保证相同的 Group By Key 被分布到同一个 Reduce 中），最后完成最终的聚合操作。</p>
</li>
<li><p>查询使用mapjoin：</p>
<p><strong>大小表Join：</strong></p>
<p>使用map join让小的维度表（1000条以下的记录条数） 先进内存。在map端完成reduce.</p>
<p><strong>大表Join大表：</strong></p>
<p>把空值的key变成一个字符串加上随机数，把倾斜的数据分到不同的reduce上，由于null值关联不上，处理后并不影响最终结果。</p>
<p><strong>MapJoin 具体用法：</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt;    select /* +mapjoin(a) */ a.id aid, name, age from a join b on a.id = b.id;</span><br><span class="line">&gt;    select /* +mapjoin(movies) */ a.title, b.rating from movies a join ratings b on a.movieid =b.movieid;</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>
</li>
</ol>
</blockquote>
<blockquote>
<p>   在 hive0.11 版本以后会自动开启 map join 优化，由两个参数控制：</p>
<blockquote>
<p><strong>set hive.auto.convert.join=true; //设置 MapJoin 优化自动开启</strong></p>
<p><strong>set hive.mapjoin.smalltable.filesize=25000000 //设置小表不超过多大时开启 mapjoin 优化</strong></p>
</blockquote>
<p>   如果是大大表关联呢？那就大事化小，小事化了。<strong>把大表切分成小表，然后分别 map join</strong></p>
</blockquote>
<p>##### </p>
<h5 id="SparkShuffle"><a href="#SparkShuffle" class="headerlink" title="SparkShuffle"></a>SparkShuffle</h5><p>E:\spark-fifth\sparkSource\LYX\Spark Shuffle过程.pdf        sparkshuffle分为<strong>hashshuffle</strong>和<strong>sortshuffle</strong>以及<strong>TungstenShuffle</strong></p>
<p><strong>hashshuffle</strong>和<strong>sortshuffle</strong>区别：</p>
<blockquote>
<p>Hash Shuffle每一个reducers产生一个文件，但是Sort Shuffle只是产生一个按照reducer id排序可索引的文件，与hash Shuffle相比，sort Shuffle中每个Mapper只产生一个数据文件和一个索引文件，数据文件中的数据按照 Reducer排序，但属于同一个Reducer的数据不排序。Mapper产生的数据先放到AppendOnlyMap这个数据结构 中，如果内存不够，数据则会spill到磁盘，最后合并成一个文件。 与Hash Shuffle相比，Sort Shuffle文件数量减少，内存使用更加可控。但排序会影响速度。 </p>
</blockquote>
<p><strong>SparkShuffle以及与MRShuffle的区别:</strong></p>
<blockquote>
<p>从逻辑上面来讲，两者都是groupByKey操作，没有本质区别。</p>
<p>（1）MapReduce在Map阶段完成之后数据会被写入到内存中的一个环形缓冲区（后续的分区/分组/排序在这里完成）；Spark的Map阶段完成之后直接输出到磁盘。没有了环形缓冲区，直接输出到磁盘中<br>（2）受第一步的影响，MapReduce输出的数据是有序的（针对单个Map数据来说）；Spark的数据是无序的（可以使用RDD算子达到排序的效果）。由于在mr shuffle在环形缓冲区中进行了分区，分组，排序，所以mr的map输出的数据是有序的，spark输出的数据是无序的<br>（3）MapReduce缓冲区的数据处理完之后会spill到磁盘形成一个文件，文件数量达到阈值之后将会进行merge操作，将多个小文件合并为一个大文件；Spark没有merge过程，一个Map中如果有对应多个Reduce的数据，则直接写多个磁盘文件。spark没有merge操作，mr中多个小文件会合并成一个大文件，而spark中一个map如果对应多个reduce的数据，则直接写多个磁盘文件<br>（4）MapReduce全部通过网络来获得数据；对于本地数据Spark可以直接读取</p>
</blockquote>
<h5 id="Kafka"><a href="#Kafka" class="headerlink" title="Kafka"></a>Kafka</h5><p>kafka常见问题    E:\spark-fifth\sparkSource\LYX\kafka</p>
<p>kafka是什么？你们公司为什么用Kafka？（也即询问kafka的使用场景以及优缺点）</p>
<p>Kafka 是一个基于分布式的消息发布-订阅系统，它被设计成快速、可扩展的、持久的。与其他消息发布-订阅系统类似，Kafka 在主题当中保存消息的信息。生产者向主题写入数据，消费者从主题读取数据。由于 Kafka 的特性是支持分布式，同时也是基于分布式的，所以主题也是可以在多个节点上被分区和覆盖的。 </p>
<p>消息队列常见的使用场景有很多，但是比较核心的有 3 个：解耦、异步、削峰。</p>
<p><strong>解耦</strong>：使用 MQ，A 系统产生一条数据，发送到 MQ 里面去，哪个系统需要数据自己去 MQ 里面消费。如果新系统需要数据，直接从 MQ 里消费即可；如果某个系统不需要这条数据了，就取消对 MQ 消息的消费即可。这样下来，A 系统压根儿不需要去考虑要给谁发送数据，不需要维护这个代码，也不需要考虑人家是否调用成功、失败超时等情况。</p>
<p><strong>异步</strong>：提前将需要消费的内容放在消息队列，当需要查询等请求时直接去拉取获得即可，极大地提升操作效率</p>
<p><strong>削峰</strong>：更多地是为了解决公司业务在高峰期时的处理高并发请求问题，使用 MQ，每秒 5k 个请求写入 MQ，A 系统每秒钟最多处理 2k 个请求，因为 MySQL 每秒钟最多处理 2k 个。A 系统从 MQ 中慢慢拉取请求，每秒钟就拉取 2k 个请求，不要超过自己每秒能处理的最大请求数量就 ok，这样下来，哪怕是高峰期的时候，A 系统也绝对不会挂掉。而 MQ 每秒钟 5k 个请求进来，就 2k 个请求出去，结果就导致在中午高峰期（1 个小时），可能有几十万甚至几百万的请求积压在 MQ 中。这个短暂的高峰期积压是 ok 的 ，只要高峰期一过，A 系统就会快速将积压的消息给解决掉。 </p>
<p>原文：<a href="https://blog.csdn.net/Dome_/article/details/84990563" target="_blank" rel="noopener">https://blog.csdn.net/Dome_/article/details/84990563</a> </p>
<p>kafka写数据流程：</p>
<ul>
<li><p>producer 先从 zookeeper 的 “/brokers/…/state” 节点找到该 partition 的 leader </p>
</li>
<li><p>producer 将消息发送给该 leader </p>
</li>
<li><p>leader 将消息写入本地 log</p>
</li>
<li><p>followers 从 leader pull 消息，写入本地 log 后向 leader 发送 ACK </p>
</li>
<li><p>leader 收到所有 ISR 中的 replica 的 ACK 后，增加 HW（high watermark，最后 commit 的 offffset） 并向 producer 发送 ACK</p>
<p>这里我们一定想到了各种意外的情况，比如说网络中断，某个partition宕机了等，因此，<strong>保证消息的传输</strong>有几种方式：</p>
<ol>
<li>At most once 消息可能会丢，但绝不会重复传输</li>
<li>At least one 消息绝不会丢，但可能会重复传输</li>
<li>Exactly once 每条消息肯定会被传输一次且仅传输一次</li>
</ol>
</li>
</ul>
<p><strong>数据的可靠性和一致性</strong>： </p>
<blockquote>
<p><strong>数据可靠性保证</strong><br>当Producer向Leader发送数据时,可以通过acks参数设置数据可靠性的级别</p>
<p>0: 不论写入是否成功,server不需要给Producer发送Response,如果发生异常,server会终止连接,触发Producer更新meta数据;<br>1: Leader写入成功后即发送Response,此种情况如果Leader fail,会丢失数据<br>-1: 等待所有ISR接收到消息后再给Producer发送Response,这是最强保证<br>仅设置acks=-1也不能保证数据不丢失,当Isr列表中只有Leader时,同样有可能造成数据丢失。要保证数据不丢除了设置acks=-1, 还要保 证ISR的大小大于等于2,具体参数设置:</p>
<p><strong>1.request.required.acks:设置为-1 等待所有ISR列表中的Replica接收到消息后采算写成功;</strong></p>
<p><strong>2.min.insync.replicas: 设置为大于等于2,保证ISR中至少有两个Replica</strong> </p>
<p>Producer要在吞吐率和数据可靠性之间做一个权衡</p>
<p><strong>数据的一致性</strong></p>
<p>一致性定义:若某条消息对Consumer可见,那么即使Leader宕机了,在新Leader上数据依然可以被读到</p>
<p>1.HighWaterMark简称HW: Partition的高水位，取一个partition对应的ISR中最小的LEO（日志末端位移(log end offset)，记录了该Replica对象底层日志(log字段)中下一条消息的位移值 ）作为HW，消费者最多只能消费到HW所在的位置，另外每个replica都有highWatermark，leader和follower各自负责更新自己的highWatermark状态，highWatermark &lt;= leader. LogEndOffset<br>2.<strong>对于Leader新写入的msg，Consumer不能立刻消费，Leader会等待该消息被所有ISR中的replica同步后,更新HW,此时该消息才能被Consumer消费</strong>，即Consumer最多只能消费到HW位置</p>
<p>这样就保证了如果Leader Broker失效,该消息仍然可以从新选举的Leader中获取。对于来自内部Broker的读取请求,没有HW的限制。同时,Follower也会维护一份自己的HW,Folloer.HW = min(Leader.HW, Follower.offset)</p>
<p>原文：<a href="https://blog.csdn.net/lizhitao/article/details/52296102" target="_blank" rel="noopener">https://blog.csdn.net/lizhitao/article/details/52296102</a> </p>
</blockquote>
<h5 id="Spark调优"><a href="#Spark调优" class="headerlink" title="Spark调优"></a>Spark调优</h5><p>Spark的瓶颈一般来自于集群（stabdalone、yarn、mesos、k8s）的资源紧张，CPU，网络带宽、内存。通过都会将数据序列化，降低其内存memory和网络带宽shuffle的消耗。</p>
<blockquote>
<p>Spark的性能，想要它快，就得充分利用好系统资源，尤其是内存和CPU：核心思想就是<strong>能够用内存cache就别spill落磁盘，CPU能并行就别串行，数据能local就别shuffle</strong></p>
</blockquote>
<p>spark调优主要分：开发调优、资源参数调优、数据倾斜调优、shuffle调优、其他优化项</p>
<blockquote>
<ol>
<li><p>开发调优：避免创建重复RDD、尽可能复用同一个RDD、对多次使用的RDD进行持久化、尽量避免使用shuffle算子（shuffle算子如distinct（实际调用reduceByKey）、reduceByKey、aggregateByKey、sortByKey、groupByKey、join、cogroup、repartition等 ）、使用map-side预聚合的shuffle操作（reduceByKey(combiner)，groupByKey(没有combiner) ）、广播大变量、使用Kryo优化序列化性能</p>
</li>
<li><p>资源参数调优：</p>
<p>executor配置</p>
<ul>
<li>spark.executor.memory</li>
<li>spark.executor.instances</li>
<li>spark.executor.cores</li>
</ul>
<p>driver配置</p>
<ul>
<li>spark.driver.memory（如果没有collect操作，一般不需要很大，1~4g即可）</li>
<li>spark.driver.cores</li>
</ul>
<p>并行度</p>
<ul>
<li>spark.default.parallelism (used for RDD API)</li>
<li>spark.sql.shuffle.partitions (usef for DataFrame/DataSet API)</li>
</ul>
<p>网络超时</p>
<ul>
<li>spark.network.timeout (所有网络交互的默认超时)</li>
</ul>
<p>JVM/gc配置</p>
<ul>
<li>spark.executor.extraJavaOptions</li>
<li>spark.driver.extraJavaOptions</li>
</ul>
</li>
<li><p>数据倾斜（主要就是对key的分布不均处理）调优：</p>
<ul>
<li>使用hive ETL预处理数据：治标不治本（利用了mr的走disk特性），还多了一条skew pipeline</li>
<li>过滤少数导致倾斜的key：但有些场景倾斜是常态</li>
<li>提高shuffle操作的并行度：让每个task处理比原来更少的数据</li>
<li>将reduce join转为map join：适用于join类的shuffle，因为shuffle变map操作了</li>
</ul>
</li>
<li><p>shuffle调优：</p>
<p><strong>shuffle演进</strong></p>
<p>&lt;0.8 hashBasedShuffle</p>
<ul>
<li>每个map端的task为每个reduce端的partition/task生成一个文件，通常会产生大量的文件，伴随大量的随机磁盘IO操作与大量的内存开销<code>M*R</code></li>
</ul>
<p>1.1 引入sortBasedShuffle</p>
<ul>
<li>每个map task不会为每个reducer task生成一个单独的文件，而是会将所有的结果写到一个文件里，同时会生成一个index文件，reducer可以通过这个index文件取得它需要处理的数据<code>M</code></li>
</ul>
<p>1.4 引入Tungsten-Sort Based Shuffle</p>
<ul>
<li>亦称unsafeShuffle，将数据记录用序列化的二进制方式存储，把排序转化成指针数组的排序，引入堆外内存空间和新的内存管理模型</li>
</ul>
<p>2.0 hashBasedShuffle退出历史舞台</p>
<ul>
<li>从此Spark只有sortBasedShuffle</li>
</ul>
<p><strong>shuffle调优</strong>：</p>
<p>shuffle是一个涉及到CPU（序列化反序列化）、网络IO（跨节点数据传输）以及磁盘IO（shuffle中间结果落盘）的操作。所以用户在编写Saprk应用程序的过程中应当尽可能避免shuffle算子和考虑shuffle相关的优化，提升spark应用程序的性能。</p>
<p>要减少Shuffle的开销，主要有两个思路：</p>
<ol>
<li>减少shuffle次数，尽量不改变key，把数据处理在local完成</li>
<li>减少shuffle的数据规模</li>
</ol>
<ul>
<li><p>先去重，再合并 ：<code>A.union(B).distinct()</code>vs<code>A.distinct().union(B.distinct()).distinct()</code></p>
</li>
<li><p>用broadcast + filter来代替join</p>
</li>
<li><p>spark.shuffle.file.buffer</p>
<p>设置shuffle write task的buffer大小，将数据写到磁盘文件之前，会先写入buffer缓冲中，待缓冲写满之后，才会溢写到磁盘</p>
</li>
<li><p>spark.reducer.maxSizeInFlight</p>
<p>设置shuffle read task的buffer大小，决定了每次能够拉取pull多少数据。减少拉取数据的次数，也就减少了网络传输的次数</p>
</li>
<li><p>spark.shuffle.sort.bypassMergeThreshold</p>
<p>shuffle read task的数量小于这个阈值（默认是200），则map-side/shuffle write过程中不会进行排序操作</p>
</li>
</ul>
</li>
<li><p>增加spark任务并行度：spark作业中，各个stage得task数量，也就代表了spark作业再各个阶段stage的并行度</p>
<ul>
<li><p>官方推荐task的数量设置成spark Application总cpu core数量的2-3倍，比如150个cpu core基本设置task数量为300-500个（有的task运行快，结束后资源再利用）</p>
</li>
<li><p>设置参数：spark.default.parallelism默认没有值，是在shuffle过程才会起作用</p>
<p>new SparkConf().set(“spark.default.parallelism”,500)</p>
</li>
<li><p>如果读取的数据在HDFS上，增加block数，默认情况下split与block 是一对一的，而split又与RDD中的partition对应，所以增加了block数也就提高了并行度</p>
</li>
<li><p>RDD.repartition给RDD重新设置partition数量</p>
</li>
<li><p>spark.sql.shuffle.partitions saprk sql中shuffle过程中partition的数量</p>
</li>
</ul>
</li>
</ol>
</blockquote>
<h5 id="增加并行度以及重分区："><a href="#增加并行度以及重分区：" class="headerlink" title="增加并行度以及重分区："></a>增加并行度以及<a href="https://blog.csdn.net/wyqwilliam/article/details/82109316" target="_blank" rel="noopener">重分区</a>：</h5><p>在spark中，有时候我们觉得task并行度太小，就想着提高其并行度。<br>首先，先说一下有多少种增加分区提高并行度的方法  <strong>需要强调的是，所谓并行度，我们并不是说增加分区就是增加并行度，而是通过增加分区，运行更多的task，这样来增加并行度</strong> ：</p>
<ol>
<li><p>textFile(path, numPartion=partitionNum)</p>
</li>
<li><p>增加hdfs上的block数</p>
</li>
<li><p>reduceByKey groupByKey shuffle算子指定返回的RDD的分区数，如reduceByKey(+, 10)</p>
</li>
<li><p>自定义分区器partitionBy</p>
</li>
<li><p>重分区:</p>
<p>rdd<strong>.coalesce</strong>(partitionNum,True/False)     :其中，partitionNum表示新分区个数，True和False表示是否shuffle。需要注意的是，分区增加就必须将是否shuffle设置为True了。</p>
<p>rdd.<strong>repartition</strong>(partitionNum):repartition有一个shuffle的过程。即将每一个partition中的数据，循环遍历，分到对应的分区中。同理，对应到减小分区也是这样操作。</p>
<p>rdd.<strong>repartitionAndSortWithinPartitions</strong>（partitioner）</p>
<p>该方法根据partitioner对RDD进行分区，并且在每个结果分区中按key进行排序。</p>
<p><strong>总结</strong>：</p>
<p>如果想要增加rdd的分区，必须使用带有shuffle的重分区方式，repartition/coalesce(num, true)</p>
<p>如果想要减少rdd的分区，可以不使用带有shuffle的重分区方式，coalesce(num, false) </p>
</li>
</ol>
<h5 id="flume拦截器作用："><a href="#flume拦截器作用：" class="headerlink" title="flume拦截器作用："></a>flume拦截器作用：</h5><p>Flume中的拦截器（interceptor），用户Source读取events发送到Sink的时候，在events header中加入一些有用的信息，或者对events的内容进行过滤，完成初步的数据清洗 以及分流作用。</p>
<h5 id="flume断点续传"><a href="#flume断点续传" class="headerlink" title="flume断点续传"></a>flume断点续传</h5><p>使用flume做日志收集，flume+kafka做日志收集存储，对于历史日志来说，尽可能做到不丢不重，实际经验内网环境kafka还是很稳定可靠的，丢日志主要发生在flume重启，所以对flume进行改造，实现断点续传。主要环境及配置：jdk7，flume1.6版本，agent使用内存channel，自扩展source基于原生ExecSource改造，sink由KafkaSink改造。</p>
<p>设计思路：</p>
<ul>
<li>在sink往kafka发送完后，记录原始日志文件及行号信息到record文件（记录文件内容只有几个字节，内容为：debug.log,2016-05-18-15,265。第二个字段为时间槽或者文件的最后修改的long型毫秒时间） </li>
<li>每次重启时，在source中检查record文件是否存在，存在的话读取上次最后一条日志的文件名和行号</li>
<li>如果第二步的record文件记录和当前最新的需要抽取日志文件是同一个，且行数差距在1000以内（具体可设置），直接tail -f -n + ${ 上次抽取完的行号+1} –pid ${flume的进程号}</li>
<li>如果第二步的record文件记录和当前最新的需要抽取日志文件是同一个且行数差距在1000以上，或者record文件记录里原始日志文件名和当前的不是同一个，直接tail -f -n + ${文件的最新行号+1}debug.log,另外记录这段落下的日志信息到快照snapshot，快照文件内容为：debug.log,2016-05-18-14,5432,debug.log,2016-05-18-15,1234</li>
<li>在source中启动tail命令后，检查是否有快照文件snapshot，如果有则开启快照文件记录的日志读取直接写到kafaka</li>
</ul>
<h5 id="sqoop增量导入参数"><a href="#sqoop增量导入参数" class="headerlink" title="sqoop增量导入参数"></a><strong>sqoop增量导入参数</strong></h5><p>增量数据导入分两种，一是基于递增列的增量数据导入（<strong>Append方式</strong>）。</p>
<table>
<thead>
<tr>
<th style="text-align:center">参数</th>
<th style="text-align:center">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">–incremental append</td>
<td style="text-align:center">基于递增列的增量导入（将递增列大于阈值的所有数据增量导入）</td>
</tr>
<tr>
<td style="text-align:center">–check-column</td>
<td style="text-align:center">递增列（int） –check-column order_id \</td>
</tr>
<tr>
<td style="text-align:center">–last-value</td>
<td style="text-align:center">阈值（int） –last-value 5201314</td>
</tr>
</tbody>
</table>
<p>二是基于时间列的增量数据导入（<strong>LastModified方式</strong>） </p>
<table>
<thead>
<tr>
<th style="text-align:center">参数</th>
<th style="text-align:center">说明/例子</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">–incremental lastmodified</td>
<td style="text-align:center">基于时间列的增量导入（将时间列大于等于阈值的所有数据增量导入）</td>
</tr>
<tr>
<td style="text-align:center">–check-column</td>
<td style="text-align:center">时间列（int） –check-column time \</td>
</tr>
<tr>
<td style="text-align:center">–last-value</td>
<td style="text-align:center">阈值（int） –last-value “2014-11-09 21:00:00”</td>
</tr>
<tr>
<td style="text-align:center">–merge-key</td>
<td style="text-align:center">合并列（主键，合并键值相同的记录） –merge-key order_id \</td>
</tr>
</tbody>
</table>
<p><strong>并发导入参数如何设置：</strong></p>
<p>我们知道通过-m参数能够设置导入数据的map任务数量，即指定了-m即表示导入方式为并发导入，这时我们必须同时指定–split-by参数指定根据哪一列实现哈希分片，从而将不同分片的数据分发到不同的map任务上去跑，避免数据倾斜</p>
<p>重要tip：</p>
<ul>
<li><p>生产环境中，为了防止主库被Sqoop抽崩，我们一般从备库中抽取数据</p>
</li>
<li><p>一般RDBMS的导出速度控制在60-80MB/s，每个map任务的处理速度5-10MB/s估算，即-m参数一般设置4-8表示启动4-8个map任务并发抽取</p>
</li>
</ul>
<h5 id="缓慢变化维："><a href="#缓慢变化维：" class="headerlink" title="缓慢变化维："></a><strong>缓慢变化维</strong>：</h5><p><a href="https://www.cnblogs.com/xqzt/p/4472005.html" target="_blank" rel="noopener">https://www.cnblogs.com/xqzt/p/4472005.html</a> </p>
<p>在从 OLTP 业务数据库向 DW 数据仓库抽取数据的过程中，特别是第一次导入之后的每一次增量抽取往往会遇到这样的问题：业务数据库中的一些数据发生了更改，到底要不要将这些变化也反映到数据仓库中？在数据仓库中，哪些数据应该随之变化，哪些可以不用变化？考虑到这些变化，在数据仓库中的维度表又应该如何设计以满足这些需要。 </p>
<p>三种类型设计解决方法在对应维度表中（例：Customer维表）：</p>
<ul>
<li><p><strong>Type 1 SCD</strong> :不记录历史数据,新数据覆盖旧数据</p>
</li>
<li><p><strong>Type 2 SCD:</strong> 保存多条记录,直接新添一条记录，同时保留原有记录，并用单独的专用的字段保存区别（设置代理键：DWID - Surrogate Key 代理键，一般设置为 DW 维度表的主键，用来在数据仓库内部中的维度表和事实表建立关联。光这样设置了新的 Surrogate Key - DWID 是不够的，因为还需要告诉数据仓库哪一条信息是现在正在使用的。因此可以额外一个标志表示这条数据是最新更改的（current：0 or 1），外的一种方式就是通过起始时间来标识，Valid To 为 NULL 的标识当前数据。 ）</p>
</li>
<li><p><strong>Type 3 SCD：</strong>添加历史列，用不同的字段保存变化痕迹.它只能保存两次变化记录.适用于变化不超过两次的维度。</p>
</li>
</ul>
<h5 id="Hive函数："><a href="#Hive函数：" class="headerlink" title="Hive函数："></a>Hive函数：</h5><ol>
<li>nvl(expr1,expr2):如果第一个参数为空那么显示第二个参数的值，如果第一个参数的值不为空，则显示第一个参数本来的值</li>
<li>nvl2(expr1,expr2,expr3)：如果第一个参数为空，则显示第二个参数值，如果第一个参数不为空，则显示第三个参数值</li>
<li>regexp_extract(string subject, string pattern, int index) ：将字符串subject按照pattern正则表达式的规则拆分，返回index指定的字符。 hive&gt; select regexp_extract(‘foothebar’, ‘foo(.*?)(bar)’, 1) from lxw_dual;  结果：the</li>
<li></li>
</ol>
<h6 id="开窗函数"><a href="#开窗函数" class="headerlink" title="开窗函数"></a>开窗函数</h6><p>原文：<a href="https://blog.csdn.net/wangpei1949/article/details/81437574" target="_blank" rel="noopener">https://blog.csdn.net/wangpei1949/article/details/81437574</a> </p>
<blockquote>
<p>普通的聚合函数聚合的行集是组,开窗函数聚合的行集是窗口。因此,普通的聚合函数每组(Group by)只返回一个值，而开窗函数则可为窗口中的每行都返回一个值。简单理解，就是对查询的结果多出一列，这一列可以是聚合值，也可以是排序值。<br>开窗函数一般分为两类,<strong>聚合开窗函数和排序开窗函数</strong>。</p>
</blockquote>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//聚合开窗函数以 count 开窗函数为例：</span></span><br><span class="line">select studentId,math,departmentId,classId,</span><br><span class="line">-- 以符合条件的所有行作为窗口</span><br><span class="line">count(math) over() as count1,</span><br><span class="line"> -- 以按classId分组的所有行作为窗口</span><br><span class="line">count(math) over(partition by classId) as count2,</span><br><span class="line"> -- 以按classId分组、按math排序的所有行作为窗口</span><br><span class="line">count(math) over(partition by classId order by math) as count3,</span><br><span class="line"> -- 以按classId分组、按math排序、按 当前行+往前<span class="number">1</span>行+往后<span class="number">2</span>行的行作为窗口</span><br><span class="line">count(math) over(partition by classId order by math rows between <span class="number">1</span> preceding and <span class="number">2</span> following) as count4</span><br><span class="line">from student_scores where departmentId=<span class="string">'department1'</span>;</span><br><span class="line">结果</span><br><span class="line">studentid   math    departmentid    classid count1  count2  count3  count4</span><br><span class="line"><span class="number">111</span>         <span class="number">69</span>      department1     class1  <span class="number">9</span>       <span class="number">5</span>       <span class="number">1</span>       <span class="number">3</span></span><br><span class="line"><span class="number">113</span>         <span class="number">74</span>      department1     class1  <span class="number">9</span>       <span class="number">5</span>       <span class="number">2</span>       <span class="number">4</span></span><br><span class="line"><span class="number">112</span>         <span class="number">80</span>      department1     class1  <span class="number">9</span>       <span class="number">5</span>       <span class="number">3</span>       <span class="number">4</span></span><br><span class="line"><span class="number">115</span>         <span class="number">93</span>      department1     class1  <span class="number">9</span>       <span class="number">5</span>       <span class="number">4</span>       <span class="number">3</span></span><br><span class="line"><span class="number">114</span>         <span class="number">94</span>      department1     class1  <span class="number">9</span>       <span class="number">5</span>       <span class="number">5</span>       <span class="number">2</span></span><br><span class="line"><span class="number">124</span>         <span class="number">70</span>      department1     class2  <span class="number">9</span>       <span class="number">4</span>       <span class="number">1</span>       <span class="number">3</span></span><br><span class="line"><span class="number">121</span>         <span class="number">74</span>      department1     class2  <span class="number">9</span>       <span class="number">4</span>       <span class="number">2</span>       <span class="number">4</span></span><br><span class="line"><span class="number">123</span>         <span class="number">78</span>      department1     class2  <span class="number">9</span>       <span class="number">4</span>       <span class="number">3</span>       <span class="number">3</span></span><br><span class="line"><span class="number">122</span>         <span class="number">86</span>      department1     class2  <span class="number">9</span>       <span class="number">4</span>       <span class="number">4</span>       <span class="number">2</span></span><br><span class="line"></span><br><span class="line">结果解释:</span><br><span class="line">studentid=<span class="number">115</span>,count1为所有的行数<span class="number">9</span>,count2为分区class1中的行数<span class="number">5</span>,count3为分区class1中math值&lt;=<span class="number">93</span>的行数<span class="number">4</span>,</span><br><span class="line">count4为分区class1中math值向前+<span class="number">1</span>行向后+<span class="number">2</span>行(实际只有<span class="number">1</span>行)的总行数<span class="number">3</span>。</span><br><span class="line"><span class="comment">//first_value:返回分区中的第一个值。</span></span><br><span class="line"><span class="comment">//lag开窗函数：</span></span><br><span class="line">lag(col,n,<span class="keyword">default</span>) 用于统计窗口内往上第n个值。</span><br><span class="line">	col:列名</span><br><span class="line">	n:往上第n行</span><br><span class="line">	<span class="keyword">default</span>:往上第n行为NULL时候，取默认值,不指定则取NULL</span><br><span class="line"><span class="comment">//lead开窗函数：</span></span><br><span class="line">lead(col,n,<span class="keyword">default</span>) 用于统计窗口内往下第n个值。</span><br><span class="line">    col:列名</span><br><span class="line">    n:往下第n行</span><br><span class="line">    <span class="keyword">default</span>:往下第n行为NULL时候，取默认值,不指定则取NULL</span><br><span class="line"><span class="comment">//排名函数：row_number rank dense_rank区别</span></span><br><span class="line">row_number函数返回一个唯一的值，当碰到相同数据时，排名按照记录集中记录的顺序依次递增。</span><br><span class="line">rank函数会返回数据项在分组中的排名，排名相等会在名次中留下空位</span><br><span class="line">dense_rank返回数据项在分组中的排名，排名相等会在名次中不会留下空位</span><br></pre></td></tr></table></figure>
<h5 id="Mysql"><a href="#Mysql" class="headerlink" title="Mysql"></a>Mysql</h5><ol>
<li><p>logServer是一个日志服务器，而Nginx是日志服务器的一个服务，能进行负载均衡也能当作日志收集来使用</p>
</li>
<li><p><strong>Mysql的主从架构以及MySQL中有六种日志文件：</strong> </p>
<p><strong>重做日志</strong>（redo log）：确保事务的持久性。防止在发生故障的时间点，尚有脏页未写入磁盘，在重启mysql服务的时候，根据redo log进行重做，从而达到事务的持久性这一特性。</p>
<p><strong>回滚日志</strong>（undo log）：保存了事务发生之前的数据的一个版本，可以用于回滚，同时可以提供多版本并发控制下的读（MVCC），也即非锁定读 </p>
<p><strong>二进制日志</strong>（binlog）：用于复制，在主从复制中，从库利用主库上的binlog进行重播，实现主从同步；</p>
<p>用于数据库的基于时间点的还原；</p>
<p>错误日志（errorlog）、慢查询日志（slow query log）、一般查询日志（general log），中继日志（relay log）。 </p>
</li>
</ol>
<h5 id="常见shell命令"><a href="#常见shell命令" class="headerlink" title="常见shell命令"></a>常见shell命令</h5><ul>
<li><p>Linux awk命令</p>
<p>AWK是一种处理文本文件的语言，是一个强大的文本分析工具</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//awk [选项参数] 'script' var=value file(s) 或者 awk [选项参数] -f scriptfile var=value file(s)</span></span><br><span class="line"><span class="comment">//每行按空格或TAB分割，输出文本中的1、4项</span></span><br><span class="line">$ awk <span class="string">'&#123;print $1,$4&#125;'</span> log.txt</span><br><span class="line"><span class="comment">//awk -v设置变量</span></span><br><span class="line">$ awk -va=<span class="number">1</span> <span class="string">'&#123;print $1,$1+a&#125;'</span> log.txt</span><br></pre></td></tr></table></figure>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//然后我想删除除了包含字符串aa外所有的文件，也就是想留下aabb，aaaaaaaa，这两个文件，其他的全部删除 下面是我的命令：</span></span><br><span class="line"><span class="comment">//rm删除除去指定文件的剩余所有文件  (rm 反向删除）：</span></span><br><span class="line">rm `ls | grep -v <span class="string">"aa"</span>`</span><br><span class="line"></span><br><span class="line">wc -l file	<span class="comment">//查看一个文件的字符个数</span></span><br><span class="line"> </span><br><span class="line">grep pattern -rn /x/xx	<span class="comment">//查看/x/xx目录以及子目录下包含pattern的所有行号以及行数据</span></span><br><span class="line">-r 是递归查找，意思就是也查找当前目录的子目录中的文件</span><br><span class="line">-n 是显示行号</span><br><span class="line">-i 忽略大小写</span><br><span class="line">-l 列出匹配的文件名 </span><br><span class="line">-L 列出不匹配的文件名</span><br><span class="line">-w 只匹配整个单词，而不是字符串的一部分（如只匹配‘man’，包括man两边有符号的如‘.man.’，或者是‘=man=’而不包括‘ woman ’或者是‘ manly ’）</span><br><span class="line">原文：https:<span class="comment">//blog.csdn.net/xia7139/article/details/39309701 </span></span><br><span class="line"></span><br><span class="line">sed -i <span class="string">"s/old/new/g"</span> file*.txt 	<span class="comment">//可以将file*.txt中的所有old换成new。如果new什么都没有，就是表示删除old的意思。</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<h5 id="hive的几种启动方式："><a href="#hive的几种启动方式：" class="headerlink" title="hive的几种启动方式："></a>hive的几种启动方式：</h5><blockquote>
<ol>
<li><p>使用本地的metastore，直接通过bin/hive命令启动，默认是启动client服务，相当于bin/hive –service cli 命令</p>
<p>hive-site.xml文件中配置使用本地mysql数据库存储metastore</p>
</li>
<li><p>使用远程的metastore，metastore可以和hive客户端不在一个机器上</p>
<p>需要在hive客户端hive-site.xml文件配置需要远程连接的metastore ip地址，然后在hive客户端启动bin/hive，此时不需要hive启动本地server</p>
</li>
<li><p>启动hiveserver2，使其他服务可以通过thrift接入hive</p>
<p>需要配置相关参数然后重启hadoop再启动hiveserver2</p>
<p>启动hiveserver2：$HIVE_HOME/bin/hive –service hiveserver2</p>
<p>beeline工具测试使用jdbc方式连接：$HIVE_HOME/bin/beeline -u jdbc:hive2://localhost:10000</p>
</li>
<li><p>另外还可以通过启动hiveWebInterface，通过网页访问hive</p>
</li>
<li><p>或者不进入hive，使用hive -e（命令）或者hive -f（文件）</p>
</li>
</ol>
</blockquote>
<h5 id="创建DataFrame的方式："><a href="#创建DataFrame的方式：" class="headerlink" title="创建DataFrame的方式："></a>创建DataFrame的方式：</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//定义样例类</span></span><br><span class="line"><span class="function"><span class="keyword">case</span> class <span class="title">Person</span><span class="params">(id:Long,name:String,age:Int,faceValue:Double)</span></span></span><br><span class="line"><span class="function"><span class="comment">//测试类</span></span></span><br><span class="line"><span class="function">object CreateDataFrameV1 </span>&#123;</span><br><span class="line">  <span class="function">def <span class="title">main</span><span class="params">(args: Array[String])</span>: Unit </span>= &#123;</span><br><span class="line">    <span class="comment">//1、创建SparkSession</span></span><br><span class="line">    val sparkSession: SparkSession = SparkSession.builder()</span><br><span class="line">      .appName(<span class="string">"test"</span>)</span><br><span class="line">      .master(<span class="string">"local[2]"</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line">    <span class="comment">//2、创建RDD</span></span><br><span class="line">    val rdd: RDD[String] = sparkSession.sparkContext.textFile(<span class="string">"D://person.txt"</span>)</span><br><span class="line">    <span class="comment">//通过反射的方式创建DataFrame需要创建样例类</span></span><br><span class="line">    val personRDD: RDD[Person] = rdd.map(line =&gt; &#123;</span><br><span class="line">      val fields: Array[String] = line.split(<span class="string">","</span>)</span><br><span class="line">      val id = fields(<span class="number">0</span>).toLong</span><br><span class="line">      val name = fields(<span class="number">1</span>)</span><br><span class="line">      val age = fields(<span class="number">2</span>).toInt</span><br><span class="line">      val faceValue = fields(<span class="number">3</span>).toDouble </span><br><span class="line">      Person(id, name, age, faceValue)</span><br><span class="line">    &#125;)</span><br><span class="line">    <span class="comment">//3、导入隐式值将RDD转化为DataFrame</span></span><br><span class="line">    <span class="keyword">import</span> sparkSession.implicits._</span><br><span class="line">    val dataFrame: DataFrame = personRDD.toDF</span><br><span class="line">    <span class="comment">//4、注册一个视图</span></span><br><span class="line">    dataFrame.createOrReplaceTempView(<span class="string">"t_person"</span>)</span><br><span class="line">    <span class="comment">//5、写sql语句进行执行</span></span><br><span class="line">    val result: DataFrame = sparkSession.sql(<span class="string">"select * from t_person order by age asc,faceValue desc"</span>)</span><br><span class="line">    <span class="comment">//6、也是懒执行，需要action来触发，查看sql结果</span></span><br><span class="line">    result.show()</span><br><span class="line">    sparkSession.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">object CreateDataFrameV2 &#123;</span><br><span class="line">  <span class="function">def <span class="title">main</span><span class="params">(args: Array[String])</span>: Unit </span>= &#123;</span><br><span class="line">    val sparkSession: SparkSession = SparkSession.builder()</span><br><span class="line">      .master(<span class="string">"local"</span>)</span><br><span class="line">      .appName(<span class="string">"Create DataFrame v2"</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line">    <span class="comment">//1、读取本地文件，创建RDD</span></span><br><span class="line">    val rdd: RDD[String] = sparkSession.sparkContext.textFile(<span class="string">"D://person.txt"</span>)</span><br><span class="line">    <span class="comment">//2、整理数据</span></span><br><span class="line">    val personRDD: RDD[Row] = rdd.map(line =&gt; &#123;</span><br><span class="line">      val fields: Array[String] = line.split(<span class="string">","</span>)</span><br><span class="line">      val id = fields(<span class="number">0</span>).toLong</span><br><span class="line">      val name = fields(<span class="number">1</span>)</span><br><span class="line">      val age = fields(<span class="number">2</span>).toInt</span><br><span class="line">      val faceValue = fields(<span class="number">3</span>).toDouble</span><br><span class="line">      Row(id, name, age, faceValue)</span><br><span class="line">    &#125;)</span><br><span class="line">    <span class="comment">//3、定义结构化信息</span></span><br><span class="line">    val struct = StructType(List(</span><br><span class="line">      StructField(<span class="string">"id"</span>, LongType, <span class="keyword">true</span>),</span><br><span class="line">      StructField(<span class="string">"name"</span>, StringType, <span class="keyword">true</span>),</span><br><span class="line">      StructField(<span class="string">"age"</span>, IntegerType, <span class="keyword">true</span>),</span><br><span class="line">      StructField(<span class="string">"faceValue"</span>, DoubleType, <span class="keyword">true</span>)</span><br><span class="line">    ))</span><br><span class="line">    <span class="comment">//4、将结构化信息和RDD整和在一起创建DataFrame</span></span><br><span class="line">    val dataFrame: DataFrame = sparkSession.createDataFrame(personRDD,struct)</span><br><span class="line">    <span class="comment">//5、注册临时试图</span></span><br><span class="line">    dataFrame.createOrReplaceTempView(<span class="string">"t_person"</span>)</span><br><span class="line">    <span class="comment">//6、执行语句</span></span><br><span class="line">    val result = sparkSession.sql(<span class="string">"select * from t_person order by age asc,faceValue desc"</span>)</span><br><span class="line">    <span class="comment">//7、展示查询结果</span></span><br><span class="line">    result.show()</span><br><span class="line"></span><br><span class="line">    sparkSession.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="什么是Hadoop？"><a href="#什么是Hadoop？" class="headerlink" title="什么是Hadoop？"></a>什么是Hadoop？</h5><p>（hadoop经典面试题：E:\面试prepare\Hadoop经典面试题.pdf）</p>
<p>Hadoop 是一个能够对大量数据进行分布式处理的软件框架。具有可靠、高效、可伸缩的特点。 </p>
<p>Hadoop的核心是HDFS，分布式文件存储系统和MapReduce，分布式计算框架以及Yarn资源调度管理平台。</p>
<p>hadoop的集群是基于master/slave模式，namenode和jobtracker属于master，datanode和tasktracker属于slave， 所以一般hadoop都会有的守护进程即这四个。</p>
<h5 id="Spark基本构架以及原理"><a href="#Spark基本构架以及原理" class="headerlink" title="Spark基本构架以及原理"></a>Spark基本构架以及原理</h5><p>（原文：<a href="https://blog.csdn.net/databatman/article/details/53023818" target="_blank" rel="noopener">https://blog.csdn.net/databatman/article/details/53023818</a> ）</p>
<p>Apache Spark 是一个快速的, 多用途的集群计算系统。<strong>速度快</strong>（基于内存，实现了高效的DAG执行引擎），<strong>易用</strong>（支持Java，Scala，Python等Api），<strong>通用</strong>（Spark提供了统一的解决方案。Spark可以用于批处理、交互式查询（Spark SQL）、实时流处理（Spark Streaming）、机器学习（Spark MLlib）和图计算（GraphX）。这些不同类型的处理都可以在同一个应用中无缝 使用。），<strong>兼容性</strong>（Spark可以非常方便地与其他的开源产品进行融合。比如，Spark可以使用Hadoop的YARN和Apache Mesos作为 它的资源管理和调度器，器，并且可以处理所有Hadoop支持的数据）</p>
<p><strong>系统架构</strong>：</p>
<p>应用程序(Application): 基于Spark的用户程序，包含了一个Driver Program 和集群中多个的Executor；<br>驱动(Driver): 运行Application的main()函数并且创建SparkContext;<br>执行单元(Executor): 是为某Application运行在Worker Node上的一个进程，该进程负责运行Task，并且负责将数据存在内存或者磁盘上，每个Application都有各自独立的Executors;<br>集群管理程序(Cluster Manager): 在集群上获取资源的外部服务(例如：Local、Standalone、Mesos或Yarn等集群管理系统)；</p>
<p>操作(Operation): 作用于RDD的各种操作分为Transformation和Action</p>
<p><strong>原理</strong>一般回答底层运行机制：</p>
<ol>
<li>我们使用spark-submit提交一个spark作业之后，这个作业就会启动一个Driver进程（根据部署模式deploy-mode不同，可能在本地也可能在集群启动），然后向集群管理器申请作业所需资源，启动一定数据Executor进程</li>
<li>申请资源之后，Driver开始调度和执行编写的代码，划分stage执行一部分代码片段，每个stage创建一批Task，然后分配到各个Executor执行。每个stage执行完之后，下一个stage的Task输入数据就是上一个stage输出的中间结果，循环往复，直至代码逻辑全部执行完毕</li>
<li>Spark是根据shuffle类的算子进行stage的划分，由于有shuffle的存在，只能在parent RDD处理完成后，才能开始接下来的计算，所以需要进行stage的划分</li>
<li>Task的执行速度是跟每个Executor进程的CPU core数量有直接关系的。一个CPU core同一时间只能执行一个线程。而每个Executor进程上分配到的多个Task，都是以每个Task一条线程的方式，多线程并发运行的 </li>
</ol>
<h5 id="kettle"><a href="#kettle" class="headerlink" title="kettle"></a>kettle</h5><blockquote>
<ol>
<li><p>什么是kettle？</p>
<p>Kettle是Pentaho公司开发的一款ETL产品，以工作流为核心，强调面向解决方案而非工具的，基于java平台的商业智能(Business Intelligence,BI)套件。  </p>
</li>
<li><p>kettle特点：</p>
<ol>
<li>适用于将多个应用系统的大批量的、异构的数据进行整合，有强大的数据转换功能。</li>
<li>高效适配多种类型的异构数据库、文件和应用系统。</li>
<li>快速构建复杂大数据集中应用、无需编码。</li>
</ol>
</li>
<li><p>kettle组成：</p>
<p><strong>Spoon</strong> ：<strong>集成开发工具</strong>（Spoon），<strong>工作job的设计工具，</strong>可以进行流程的开发、配置、调试、部署、执行(转换、任务)，也可以对运行情况进行监控、对处理过程的日志进行查看、也可以通过接口调用方式进行远程管理。 </p>
<p><strong>Kitchen</strong>：一个独立的命令行程序，用于执行由Spoon编辑的作业。<strong>工作(job)执行器 (命令行方式)</strong> </p>
<p><strong>Carte</strong>：中间是服务器(<strong>Carte</strong>)，包括实际执行转换和任务的ETL引擎、监控管理的接口、认证授权接口，还有一个可以拓展的接口。 <strong>是一个轻量级的Web容器，用于建立专用、远程的ETL Server。</strong>  </p>
<p><strong>Pan</strong>：<strong>transform执行器</strong>，一个独立的命令行程序，支持通过命令行实现界面的功能，如转换启停、任务启停。状态查看等 </p>
</li>
</ol>
</blockquote>
<h5 id="MR全流程："><a href="#MR全流程：" class="headerlink" title="MR全流程："></a>MR全流程：</h5><ol>
<li>在进入map阶段之前会使用inputFormat对小文件进行合并</li>
<li>maptask收集map输出的&lt;key,value&gt;交给环形缓冲区</li>
<li>环形缓冲区大小默认为100M，当达到80%的时候，溢出文件到本地磁盘上，根据分区会溢出多个文件</li>
<li>多个溢写的小文件会被合并成一个大文件</li>
<li>在文件溢写的过程中，会根据key对文件进行分区和排序</li>
<li>分区的规则是：按照key.hashcode%reduceNumber的结果进行分区</li>
<li>reduceTask会把来自不同的maptask的分区文件加载到本地缓存中，进行归并排序并且分组，相同的key为一组</li>
<li>合并为大文件之后，shuffle的过程就结束了</li>
<li>利用outputFormat将文件转为指定格式输出</li>
</ol>
<h5 id="Yarn工作机制"><a href="#Yarn工作机制" class="headerlink" title="Yarn工作机制"></a>Yarn工作机制</h5><ol>
<li><p>client将jar包提交到yarn runner（yarn运行器）</p>
<p>job.waitForCompletion(true)</p>
</li>
<li><p>向resourcemanager申请提交jar包</p>
</li>
<li><p>RM对整个集群二点资源进行预估检查</p>
</li>
<li><p>RM回应client，返回一个路径+任务id（Application_id)，路径：/tmp/…/**.stating：云存储（HDFS)</p>
</li>
<li><p>Yarn runner将相关计算资源上传到RM返回的路径上，计算资源包括：</p>
<p>a) jar包</p>
<p>b) job.split</p>
<p>c) job.xml(core-site.xml,hdfs-site.xml,yarn-site.xml,mapred-site.xml)</p>
</li>
<li><p>通知RM相关资源已经上传完毕</p>
</li>
<li><p>开启任务，将任务加入队列</p>
</li>
<li><p>把任务分配给剩余资源多的节点</p>
</li>
<li><p>HDFS把相关计算资源加载到MRAppMstr中</p>
</li>
<li><p>MRAppMstr向RM申请资源</p>
</li>
<li><p>RM通知拥有数据块的NM创建Container容器</p>
</li>
<li><p>运行成功之后将报告提交给MRAppMstr</p>
</li>
<li><p>MRAppMstr申请reduce资源</p>
</li>
<li><p>通知 执行reduce的NM创建container容器</p>
</li>
<li><p>将map归并排序之后形成的中间文件下载到reduce节点上</p>
</li>
<li><p>执行结束之后向MRAppMstr报告任务结束</p>
</li>
<li><p>MRAppMstr通知RM，MR程序已经完成了，请求释放资源，自动销毁</p>
</li>
<li><h5 id="HA机制：C-Users-T-whong-Desktop-QS-FS-面试宝典76"><a href="#HA机制：C-Users-T-whong-Desktop-QS-FS-面试宝典76" class="headerlink" title="HA机制：C:\Users\T-whong\Desktop\QS\FS\面试宝典76"></a>HA机制：C:\Users\T-whong\Desktop\QS\FS\面试宝典76</h5></li>
</ol>
<h5 id="各种排序算法的使用："><a href="#各种排序算法的使用：" class="headerlink" title="各种排序算法的使用："></a>各种排序算法的使用：</h5><p>E:\面试prepare\各种排序算法的使用范围《仅供参考》.pdf</p>
<p>快排：<a href="https://juejin.im/post/5b55660ee51d4519202e2003" target="_blank" rel="noopener">https://juejin.im/post/5b55660ee51d4519202e2003</a> <strong>挖坑填数 + 分治法</strong></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">快速排序使用分治法策略来把一个序列分为两个子序列，基本步骤为：</span><br><span class="line"><span class="number">1</span>. 先从序列中取出一个数作为基准数；</span><br><span class="line"><span class="number">2</span>. 分区过程：将把这个数大的数全部放到它的右边，小于或者等于它的数全放到它的左边；</span><br><span class="line"><span class="number">3</span>. 递归地对左右子序列进行步骤<span class="number">2</span>，直到各区间只有一个数。</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Test09</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">printArr</span><span class="params">(<span class="keyword">int</span>[] arr)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> anArr : arr) &#123;</span><br><span class="line">            System.out.print(anArr + <span class="string">" "</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">int</span> <span class="title">partition</span><span class="params">(<span class="keyword">int</span>[] arr, <span class="keyword">int</span> left, <span class="keyword">int</span> right)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> temp = arr[left];</span><br><span class="line">        <span class="keyword">while</span> (right &gt; left) &#123;</span><br><span class="line">            <span class="comment">// 先判断基准数和后面的数依次比较</span></span><br><span class="line">            <span class="keyword">while</span> (temp &lt;= arr[right] &amp;&amp; left &lt; right) &#123;</span><br><span class="line">                --right;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">// 当基准数大于了 arr[right]，则填坑</span></span><br><span class="line">            <span class="keyword">if</span> (left &lt; right) &#123;</span><br><span class="line">                arr[left] = arr[right];</span><br><span class="line">                ++left;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">// 现在是 arr[right] 需要填坑了</span></span><br><span class="line">            <span class="keyword">while</span> (temp &gt;= arr[left] &amp;&amp; left &lt; right) &#123;</span><br><span class="line">                ++left;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span> (left &lt; right) &#123;</span><br><span class="line">                arr[right] = arr[left];</span><br><span class="line">                --right;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        arr[left] = temp;</span><br><span class="line">        <span class="keyword">return</span> left;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">quickSort</span><span class="params">(<span class="keyword">int</span>[] arr, <span class="keyword">int</span> left, <span class="keyword">int</span> right)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (arr == <span class="keyword">null</span> || left &gt;= right || arr.length &lt;= <span class="number">1</span>)</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        <span class="keyword">int</span> mid = partition(arr, left, right);</span><br><span class="line">        quickSort(arr, left, mid);</span><br><span class="line">        quickSort(arr, mid + <span class="number">1</span>, right);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span>[] arr = &#123;<span class="number">6</span>, <span class="number">4</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">7</span>, <span class="number">9</span>, <span class="number">1</span>, <span class="number">8</span>, <span class="number">5</span>&#125;;</span><br><span class="line">        quickSort(arr, <span class="number">0</span>, arr.length - <span class="number">1</span>);</span><br><span class="line">        printArr(arr);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="Hive复杂数据类型"><a href="#Hive复杂数据类型" class="headerlink" title="Hive复杂数据类型"></a>Hive复杂数据类型</h5><p>原文：<a href="https://blog.csdn.net/sl1992/article/details/53894481" target="_blank" rel="noopener">https://blog.csdn.net/sl1992/article/details/53894481</a> </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//array用法：zhangsan    beijing,shanghai,hangzhou</span></span><br><span class="line">hive&gt; <span class="function">create table <span class="title">person_array</span><span class="params">(name string,work_locations array&lt;string&gt;)</span></span></span><br><span class="line"><span class="function">    &gt; row format delimited fields terminated by '\t'</span></span><br><span class="line"><span class="function">    &gt; collection items terminated by ','</span>;</span><br><span class="line"><span class="comment">//一些查询操作</span></span><br><span class="line"> select name,work_locations[<span class="number">0</span>],size(work_locations) from person_array；</span><br><span class="line"> <span class="function">select name from person_array where <span class="title">array_contains</span><span class="params">(work_locations,<span class="string">'nanjing'</span>)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">//map的用法：zhangsan    Math:90,Chinese:92,English:78</span></span><br><span class="line">hive&gt; <span class="function">create table <span class="title">score_map</span><span class="params">(name string,score map&lt;string,<span class="keyword">int</span>&gt;)</span></span></span><br><span class="line"><span class="function">    &gt; row format delimited fields terminated by '\t'</span></span><br><span class="line"><span class="function">    &gt; collection items terminated by ','</span></span><br><span class="line"><span class="function">    &gt; map keys terminated by ':'</span>;</span><br><span class="line"><span class="comment">//一些查询操作</span></span><br><span class="line">select name,score[<span class="string">'English'</span>],size(score) from score_map;</span><br><span class="line"><span class="function">select name from score_map where <span class="title">array_contains</span><span class="params">(map_keys(score)</span>,'Elective')</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">//struct的用法：zhangsan    Math,90</span></span><br><span class="line">hive&gt; <span class="function">create table <span class="title">course_struct</span><span class="params">(name string,course struct&lt;course:string,score:<span class="keyword">int</span>&gt;)</span></span></span><br><span class="line"><span class="function">    &gt; row format delimited fields terminated by '\t'</span></span><br><span class="line"><span class="function">    &gt; collection items terminated by ','</span>;</span><br><span class="line"><span class="comment">//一些查询操作</span></span><br><span class="line">select name,course.score,course.course from course_struct;</span><br><span class="line"></span><br><span class="line"><span class="comment">//克隆表，不带数据</span></span><br><span class="line">create table <span class="keyword">if</span> not exists t8 like t1;</span><br><span class="line"><span class="comment">//克隆表，带数据</span></span><br><span class="line">create table <span class="keyword">if</span> not exists t9 like t1 location <span class="string">'/user/hive/warehouse/gp1816.db/t1'</span></span><br><span class="line"><span class="comment">//克隆表带数据更灵活的方式</span></span><br><span class="line">create table <span class="keyword">if</span> not exists t10 as select uid,uname from t1 where uid &lt; <span class="number">5</span>； </span><br><span class="line"><span class="comment">//hive的分区本质上是在表目录下创建子目录，但是该分区字段是一个伪列不真实存在于数据中，age=19,age=20</span></span><br></pre></td></tr></table></figure>
<h5 id="Hive行列转换"><a href="#Hive行列转换" class="headerlink" title="Hive行列转换"></a>Hive行列转换</h5><h6 id="列转行："><a href="#列转行：" class="headerlink" title="列转行："></a><strong>列转行</strong>：</h6><table>
<thead>
<tr>
<th>函数</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>explode(col)</td>
<td>将hive一列中复杂的array或者map结构拆分成多行</td>
</tr>
<tr>
<td>lateral view</td>
<td>用法：lateral view udtf(expression) tabletemp as  column   用于和split，explode等UDTF一起使用，能够将一列数据拆成多行数据，在此基础上可以对拆分后的数据进行聚合</td>
</tr>
</tbody>
</table>
<p>如下表，求name出现次数top3</p>
<table>
<thead>
<tr>
<th>id(int)</th>
<th>name(string)</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>1，2，3，4，5</td>
</tr>
<tr>
<td>2</td>
<td>2，3，4，5，6</td>
</tr>
<tr>
<td>3</td>
<td>3，4，5，6，7</td>
</tr>
<tr>
<td>4</td>
<td>4，5，6，7，8</td>
</tr>
<tr>
<td>5</td>
<td>5，6，7，8，9</td>
</tr>
</tbody>
</table>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//1,建表，以空格分隔</span></span><br><span class="line"><span class="function">create table <span class="keyword">if</span> not exists <span class="title">xx</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">id <span class="keyword">int</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">name string</span></span></span><br><span class="line"><span class="function"><span class="params">)</span>row format delimited fields terminated by ' '</span>;</span><br><span class="line"><span class="comment">//2，将数据文件load导入</span></span><br><span class="line">load data local inpath <span class="string">'path'</span> into table xx;</span><br><span class="line"><span class="comment">//3，先将name字段分割为数组形式然后对name分组聚合排名最后取top3</span></span><br><span class="line">with a as </span><br><span class="line">(select id,<span class="function">name1 from xx lateral view <span class="title">explode</span><span class="params">(split(name,<span class="string">","</span>)</span>) temptab as name1),</span></span><br><span class="line"><span class="function">b as </span></span><br><span class="line"><span class="function"><span class="params">(select a.name1 as name,count(a.name1)</span> namenum from a group by a.name1), </span></span><br><span class="line"><span class="function">c as</span></span><br><span class="line"><span class="function"><span class="params">(select b.name,row_number()</span> <span class="title">over</span><span class="params">(sort by b.namenum desc)</span> as rank from b) </span></span><br><span class="line"><span class="function">select c.* from c where c.rank &lt;</span>= <span class="number">3</span>;</span><br></pre></td></tr></table></figure>
<h6 id="行转列："><a href="#行转列：" class="headerlink" title="行转列："></a>行转列：</h6><p>如下表把星座和血型一样的人归类到一起</p>
<table>
<thead>
<tr>
<th>name</th>
<th>con</th>
<th>bloodtype</th>
</tr>
</thead>
<tbody>
<tr>
<td>孙悟空</td>
<td>白羊座</td>
<td>A</td>
</tr>
<tr>
<td>大海</td>
<td>射手座</td>
<td>A</td>
</tr>
<tr>
<td>宋宋</td>
<td>白羊座</td>
<td>B</td>
</tr>
<tr>
<td>猪八戒</td>
<td>白羊座</td>
<td>A</td>
</tr>
<tr>
<td>凤姐</td>
<td>射手座</td>
<td>A</td>
</tr>
</tbody>
</table>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//需求：结果如下</span></span><br><span class="line">射手座,A            大海|凤姐</span><br><span class="line">白羊座,A            孙悟空|猪八戒</span><br><span class="line">白羊座,B            宋宋</span><br><span class="line"><span class="comment">//sql如下</span></span><br><span class="line"><span class="function">create table <span class="keyword">if</span> not exists <span class="title">yy</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">name string,</span></span></span><br><span class="line"><span class="function"><span class="params">con string,</span></span></span><br><span class="line"><span class="function"><span class="params">bloodtype string</span></span></span><br><span class="line"><span class="function"><span class="params">)</span>row format delimited fields terminated by ' '</span>;</span><br><span class="line">load data local inpath <span class="string">'/data/xx'</span> into table yy;</span><br><span class="line"></span><br><span class="line"><span class="function">with a <span class="title">as</span> <span class="params">(select name,concat(con,<span class="string">","</span>,bloodtype)</span> base from yy) </span></span><br><span class="line"><span class="function">select a.base,<span class="title">concat_ws</span><span class="params">(<span class="string">'|'</span>,collect_set(a.name)</span>) name </span></span><br><span class="line"><span class="function">from a group by a.base</span>;</span><br></pre></td></tr></table></figure>
<h5 id="Redis缓存雪崩缓存穿透"><a href="#Redis缓存雪崩缓存穿透" class="headerlink" title="Redis缓存雪崩缓存穿透"></a>Redis缓存雪崩缓存穿透</h5><p>我们通常使用缓存+过期时间的策略来帮助我们加速接口的访问速度，减少了后端负载，同时保证功能的更新</p>
<h6 id="缓存穿透："><a href="#缓存穿透：" class="headerlink" title="缓存穿透："></a>缓存穿透：</h6><p>缓存系统，按照KEY去查询VALUE，当KEY对应的VALUE一定不存在的时候并对KEY并发请求量很大的时候，就会对后端造成很大的压力。由于缓存不命中，每次都要查询持久层，从而失去缓存的意义。</p>
<p>解决方法：</p>
<ol>
<li>缓存层缓存空值<ul>
<li>缓存太多空值，占用更多空间（优化：给个空值过期时间）</li>
<li>存储层更新代码了，缓存层还是空值（优化：后台设置时主动删除空值，并把值缓存进去）</li>
</ul>
</li>
<li>将数据库中所有的查询条件，放到布隆过滤器中，当一个查询请求来临的时候，先经过布隆过滤器进行检查，如果请求在这个条件中，那么继续执行，如果不在，直接丢弃。我们可以利用布隆过滤器将redis缓存击穿控制在一个可容忍的范围内。</li>
</ol>
<h6 id="缓存雪崩（缓存失效）"><a href="#缓存雪崩（缓存失效）" class="headerlink" title="缓存雪崩（缓存失效）"></a>缓存雪崩（缓存失效）</h6><p>如果缓存集中在一段时间内失效，发生大量的缓存穿透，所有的查询都落在数据库上，造成了缓存雪崩。缓存层宕掉后，流量会像奔逃的野牛一样，打向后端存储</p>
<p>解决办法：</p>
<ol>
<li>在缓存失效后，通过加锁或者队列来控制读数据库写缓存的线程数据。比如对某个key只允许一个线程查询数据和写缓存，其他线程等待</li>
<li>可以通过缓存reload机制，预先去更新缓存，在即将发生大并发访问前手动触发加载缓存</li>
<li>不同的key，设置不同的过期时间，让缓存失效的时间点尽量均匀</li>
<li>做二级缓存，或者双缓存策略。A1为原始缓存，A2为拷贝缓存，A1失效时，可以访问A2，A1缓存失效时间设置为短期，A2设置为长期</li>
</ol>
<h6 id="热点key"><a href="#热点key" class="headerlink" title="热点key"></a>热点key</h6><p>这个key是一个热点key（例如一个重要的新闻，一个热门的八卦新闻等），所有这种key访问量可能非常大。缓存的构建是需要一定时间的。（可能是一个复杂的计算，例如复杂的sql、多次IO、多个依赖（各种接口）等）。于是就会出现一个致命问题：在缓存失效的瞬间，有大量线程来构建缓存，造成后端负载加大，甚至可能让系统崩溃</p>
<p>解决办法：</p>
<ol>
<li>使用互斥锁（mutex key）这种解决方案思路比较简单，就是只让一个线程构建缓存，其他线程等待构建缓存的线程执行完，重新从缓存获取数据就可以 了</li>
<li>”提前“使用互斥锁（mutex key）：在value内部设置1个超时值（timeout1），timeout1比实际的memcache timeout（timeout2）小，当从cache读取到timeout1发现它已经过期时候，马上延长timeout1并重新设置到cache。然后再从数据库加载数据并设置到cache中。</li>
<li>“永远不过期”<ul>
<li>从redis上看，确实没有设置过期是阿锦，这就保证了不会出现热点key过期问题，也就是物理不过期</li>
<li>从功能上看，如果不过期，那不就成静态了吗？所以我们把过期时间存在key对应的value里，如果发现要过期了，通过一个后台的异步线程进行缓存的构建，也就是逻辑的过期</li>
</ul>
</li>
<li>资源保护：可以做资源的隔离保护主线程池，如果把这个应用到缓存的构建也未尝不可。</li>
</ol>
<h3 id="简历附件："><a href="#简历附件：" class="headerlink" title="简历附件："></a><strong>简历附件：</strong></h3><h4 id="蜂首项目一："><a href="#蜂首项目一：" class="headerlink" title="蜂首项目一："></a>蜂首项目一：</h4><p>flume-NG-1.6.0 ：flumeNG(1.x)以后没有 collector、master 这两类节点。这是核心组件最核心的变化。去除了 physical nodes、logical nodes 的概念和相关内容。不再依赖zookeeper进行多类节点(OG:collector,master,agent)得稳定性管理</p>
<p>kettle：Kettle也叫PDI (Pentaho Data Intergration)     kettle-7.1</p>
<p>hive-1.2.1    spark-2.3.1    azkaban-3.56.0    hadoop-2.7.2</p>
<h5 id="项目总结："><a href="#项目总结：" class="headerlink" title="项目总结："></a>项目总结：</h5><ol>
<li><h5 id="第一个项目总结1："><a href="#第一个项目总结1：" class="headerlink" title="第一个项目总结1："></a>第一个项目总结1：</h5><ol>
<li><p>当时首先考虑是不是CPU或者内存分配不合理导致运行所需资源不够，使用top等命令查看进程以及进            程下的线程所占用的资源情况发现并无异常。</p>
</li>
<li><p>后来又考虑到是不是Kettle使用过程中对数据抽取或者转换等操作步骤的代码编写问题，由于Kettle本身工作机制是由一个个Transformation构成，所以对数据的不同操作进行隔离排除就相对来说方便一些。</p>
</li>
<li>经过对不同Transformation的排查发现是在HQL语句编写时使用到的一个Left外关联对副表的过滤条件写在了Where后面，导致先关联全表然后再进行过滤。当将对副表的过滤条件放在Left Join里面的时候，会先对副表做一次过滤然后再进行关联，这样问题解决。</li>
</ol>
</li>
<li><h5 id="第一个项目总结2："><a href="#第一个项目总结2：" class="headerlink" title="第一个项目总结2："></a>第一个项目总结2：</h5><p>这一类数据主要是一些应该有的信息缺失，如供应商的名称、分公司的名称、客户的区域信息缺失、业务系统中主表与明细表不能匹配等。对于这一类数据过滤出来，按缺失的内容分别写入不同Excel文件向客户提交，要求在规定的时间内补全。补全后才写入数据仓库。 </p>
</li>
<li><h5 id="第一个项目总结3："><a href="#第一个项目总结3：" class="headerlink" title="第一个项目总结3："></a>第一个项目总结3：</h5><p>我们把嵌套数据类型的一行叫做一个记录（record)，嵌套数据类型的特点是一个record中的column除了可以是Int, Long, String这样的原语（primitive）类型以外，还可以是List, Map, Set这样的复杂类型。在行式存储中一行的多列是连续的写在一起的，在列式存储中数据按列分开存储，例如可以只读取A.B.C这一列的数据而不去读A.E和A.B.D，那么如何根据读取出来的各个列的数据重构出一行记录呢？ 图1 行式存储和列式存储 Google的Dremel系统解决了这个问题，核心思想是使用“record shredding and assembly algorithm”来表示复杂的嵌套数据类型，同时辅以按列的高效压缩和编码技术，实现降低存储空间，提高IO效率，降低上层应用延迟。 <a href="https://max.book118.com/html/2017/1220/145080894.shtm" target="_blank" rel="noopener">https://max.book118.com/html/2017/1220/145080894.shtm</a> </p>
</li>
<li><p>azkaban失败任务重试：在.job文件添加如下：</p>
<blockquote>
<p>retries=12 </p>
<p>retry.backoff=300000  #代表重试间隔时间 </p>
</blockquote>
</li>
</ol>
<h5 id="责任描述1："><a href="#责任描述1：" class="headerlink" title="责任描述1："></a>责任描述1：</h5><ol>
<li><p>收集的HDFS日志json格式业务数据，利用parquet格式存到HDFS指定的路径，把json存储为parquet不但可以节省50%左右的存储空间，更方便后续工作中的数据查询，并且查询效率比文本方式更高，日志转为aprquet格式步骤：</p>
<ol>
<li><p>用saprk读取json数据后，得到一个dataframe</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">path = '/xx/xx/xx.json'</span><br><span class="line">df = sqlContext.read.json(path)</span><br></pre></td></tr></table></figure>
</li>
<li><p>dataframe经过filter、select等操作再以parquet格式写入hdfs指定路径</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df = df.filter(<span class="symbol">'xx</span>x').select(<span class="symbol">'xx</span>x')</span><br></pre></td></tr></table></figure>
</li>
<li><p>目录是按天分区，类似/parquet/xxxxxxx/dt=2017-06-06这样</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.write.parquet(<span class="string">"/parquet/xxxxxxx/dt=2017-06-06"</span>,<span class="string">"overwrite"</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p>在创建对应hive表的时候load和alter的主要区别：load会创建分区目录，并移动数据到分区下，alter的方式不会创建分区目录，也不会移动数据，个人场景alter方式添加分区更合适</p>
</li>
</ol>
</li>
</ol>
<ol start="2">
<li><p>得到的数据使用<strong>Kettle</strong>进行轻度ETL清洗，<strong>例如去除重复记录、脏记录</strong>，步骤如下：</p>
<p>先使用sql取出将要处理的数据字段（表输入sql） ——&gt;Sort Rows Kettle排序——&gt;Unique Rows Kettle去重——&gt;Javascript Kettle脚本 ——&gt;Field Selection选择需要的字段，去除额外的字段——&gt;进行Switch/Case选择转换的字段转换成需要的字段值（默认值为Null）—&gt;Over</p>
<p>例如：查看数据流程时：发现表输入记录个数为117081，而经过去除重复记录之后数据条数为117032 </p>
<p>Spoon是一个图形用户界面，它允许你运行转换或者任务，其中转换是用Pan工具来运行，任务是用 <strong>Kitchen</strong>来运行。<strong>Pan</strong>是一个数据转换引擎，可以执行很多功能，例如：从不同的数据源读取操作和写入数据。Kitchen是一个可以运行利用XML或数据资源库描述的任务，通常任务是在规定的时间间隔内用批处理的模式自动运行。</p>
<p><a href="https://ask.hellobi.com/blog/hql15/3834" target="_blank" rel="noopener">https://ask.hellobi.com/blog/hql15/3834</a> </p>
</li>
</ol>
<h5 id="责任描述2："><a href="#责任描述2：" class="headerlink" title="责任描述2："></a>责任描述2：</h5><p>数仓建模方法</p>
<p>目前业界较为流行的数仓建模方法很多，但主要有范式建模，维度建模，实体建模等几种方法，每种方法其实本质上就是从不同角度看我们的业务中的问题，不管从技术层面还是业务层面，其实代表的是哲学上的一种世界观。</p>
<ol>
<li><p>范式建模法（Third Normal Form,3NF）</p>
<p>1NF：字段是最小的的单元不可再分 </p>
<p>2NF：满足1NF,表中的字段必须完全依赖于全部主键而非部分主键 (一般我们都会做到) </p>
<p>3NF：满足2NF,非主键外的所有字段必须互不依赖 </p>
<p>4NF：满足3NF,消除表中的多值依赖 </p>
<p>该方法主要由Inmon所提倡，范式是数据库逻辑设计的基本理论，一个关系模型可以从第一范式到第五范式进行无损分解，这个过程也可称为规范化。在数据仓库的模型设计中目前一般采用第三范式，一个符合第三范式的关系必须具有以下三个条件：</p>
<ol>
<li>每个属性值唯一，不具有多义性；</li>
<li>每个非主属性必须完全依赖于整个主键，而非主键的一部分；</li>
<li>每个非主属性不能依赖于其他关系中的属性，因为这样的话这种属性应该归到其他关系中去。</li>
</ol>
<p>Inmon范式建模优缺点：</p>
<p>Inmon 的范式建模法的最大优点就是从关系型数据库的角度出发，结合了业务系统的数据模型，能够比较方便的实现数据仓库的建模。但其缺点也是明显的，由于建模方法限定在关系型数据库之上，在某些时候反而限制了整个数据仓库模型的灵活性，性能等，特别是考虑到数据仓库的底层数据向数据集市的数据进行汇总时，需要进行一定的变通才能满足相应的需求。</p>
</li>
<li><p>维度建模法</p>
<p>维度建模法，Kimball最先提出这一概念，按照事实表，维表来构建数据仓库、数据集市。这种方法的最被广泛知晓的名字就是星型模型（Star-schema）。</p>
<p>优点：</p>
<ul>
<li>这种模型针对各个维度做了大量的预处理，如按照维进行预先的统计、分类、排序等。通过这些预处理，能够极大的提升数据仓库的处理能力</li>
<li>维度建模非常直观，紧紧围绕着业务模型，可以直观的反映出业务模型中的业务问题，不需要经过特别的抽象处理即可完成维度建模</li>
</ul>
<p>缺点：</p>
<ul>
<li>由于在构建星型模式之前需要进行大量的数据预处理，因此会导致大量的数据处理工作。而且当<strong>业务发生变化时需要重新进行维度的定义</strong>时，往往需要重新进行维度数据的预处理，而在这些预处理过程中，往往会导致大量的数据冗余</li>
<li>如果只是依靠单纯的维度建模，不能保证数据来源的一致性和准确性，而且在数仓的底层，不是特别适用于维度建模的方法</li>
</ul>
<p>维度建模总结：维度建模的领域主要适用于数据集市层，他的最大作用其实是为了解决数仓建模中的性能问题。维度建模很难提供一个完整的描述真实业务实体之间的赋值关系的抽象方法</p>
</li>
<li><p>Kimball vs Inmon（从以下两面考虑回答为什么糅合Kimball建模以及有效时间效率问题）<a href="https://www.jianshu.com/p/b83be2643d91" target="_blank" rel="noopener">https://www.jianshu.com/p/b83be2643d91</a> </p>
<ul>
<li><p>数仓建设方式：</p>
<p><strong>Kimball建议从底向上</strong>。先建设满足部门级分析需求的若干数据集市，再通过总线架构将他们集成，形成一个“联合数据仓库”</p>
<p><strong>Inmon强调自顶向下</strong>。先将来自各源业务系统数据集成至企业级的数据仓库，再基于其搭建面向部门应用需求的数据集市。</p>
</li>
<li><p>特性对比：</p>
<p>Kimball方式对团队技术水平要求不太高，更易于实现，从小型的主题域数据集市建设起，且有<strong>维度建设预处理</strong>，但在逐步建设的过程中，联合维度数据仓库的一致性较难控制，适用于战术层级的规划，或是有迫切的目标需要实现</p>
<p>Inmon的方式，<strong>规范性较好，数据集成和数据一致性方面得到处理</strong>，适用于较为大型的企业级、战略级的规划。但对团队要求较高，且<strong>实现周期较</strong>长、成本高昂。具体可能根据企业的规模、项目规划、预算、团队等角度进行综合考虑。</p>
</li>
</ul>
</li>
</ol>
<h5 id="责任描述3："><a href="#责任描述3：" class="headerlink" title="责任描述3："></a>责任描述3：</h5><p><strong>点击流日志信息中包含的东西、ETL轻度清洗源数据后到ODS层的表，字段，以及DW层，DIM层，都有什么表，以及DM层指标的计算都有哪些，写一个复杂的指标</strong></p>
<ul>
<li><p>点击流日志信息中包含的东西</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">在一个session会话中的操作行为（浏览行为，点击行为），每个操作都会产生一条日志信息：</span><br><span class="line">URL req 、请求时间 reqTime、操作系统、操作类型 type（<span class="number">0</span>：浏览，<span class="number">1</span>：点击）、页面停留时间、sessionID等</span><br></pre></td></tr></table></figure>
</li>
<li><p>ODS层：</p>
<p>四张表（只针对于本项目的业务表，且按天分区表）：业务数据库来的两张表：用户基本信息表、订单信息宽表</p>
<p>flume拉取日志来的两张表：页面访问信息宽表、事件表（包括app端和web端）</p>
<p>字段：IP（纯真库抽取IP所在地）、客户ID、访问时间、浏览器信息等</p>
</li>
<li><p>DIM层:</p>
<p>维表共三十多张：浏览器维表dim_browser、地域维表dim_location、操作系统维表dim_os、访问平台维表dim_platform、时间维表dim_date、支付类型维表dim_payment_type等</p>
</li>
<li><p>DW层：</p>
<p>页面访问事实表、订单事实表（字段：订单金额、订单日期、订单量、订单ID等）、用户活跃事实表（字段：日期、客户ID、登陆时间、登陆次数等）等表</p>
<p>DW层数仓建模的时候采用的是星型模型，按照事实表和维表进行数仓的构建，而星型模型性能相对较高的一个原因就是针对维表进行了提前的预处理，其需要拆分的二级维度也不算很多，例如浏览器的名字下的类型以及地域维度省下的二级维度市，星型模型因为数据的冗余所以很多统计查询不需要做外部的连接，因此在冗余可以接受的前提下一般选用星型模型而不是雪花模型。</p>
</li>
<li><p>DM层：</p>
<p>指标的计算共两千多个：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">用户行为分析KPI：</span><br><span class="line">漏斗模型分析、有效行为分析（平均有多少次浏览行为产生预定行为）、登陆流失率、日访问时间分布</span><br><span class="line">用户规模KPI：</span><br><span class="line">新增用户数、累计用户数、平台维度累计用户数、忠诚用户占比</span><br><span class="line">用户活跃KPI：</span><br><span class="line">连续<span class="number">8</span>周每周至少登陆过一次的用户数、当天登陆峰值、月活等</span><br></pre></td></tr></table></figure>
<blockquote>
<p>漏斗分析模型：</p>
<p>我认为本质是分解和量化，营销漏斗模型指的是营销过程中，将非潜在客户逐步变为客户的转化量化模型。营销漏斗模型的价值在于量化了营销过程各个环节的效率，帮助找到薄弱环节。相邻环节的转化率则就是指用数据指标来量化每一个步骤的表现。</p>
<p>例如本项目酒店预订下单转化率漏斗分析模型各阶段转化率：进入首页（100）、查看商品页（70，流失率30%）、进入支付页面（20，流失率72%）、支付成功（13，整体转化率13%）</p>
</blockquote>
<p><strong>复杂指标</strong>：<strong>HIVE基于SQL创建漏斗分析模型</strong>(<a href="https://blog.csdn.net/mrbcy/article/details/66477208" target="_blank" rel="noopener">https://blog.csdn.net/mrbcy/article/details/66477208</a> )</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//概述：</span></span><br><span class="line">一般来说，客户会按照A-&gt;B-&gt;C-&gt;D的顺序来访问页面，而且越到后面的页面访问率就越低，比如A是网站首页，B是列表页，C是详情页，D是支付页，根据常识，很容易知道A页面的访问率是最高的而D则是最低的。</span><br><span class="line"><span class="comment">//需求：</span></span><br><span class="line">使用SQL语句来完成以下的工作：</span><br><span class="line"><span class="number">1</span>.找出乱序访问的用户访问记录，即不按照A-&gt;B-&gt;C-&gt;D的顺序访问的</span><br><span class="line"><span class="number">2</span>.找出顺序访问的用户访问记录，即严格按照A-&gt;B-&gt;C-&gt;D的顺序访问的</span><br><span class="line"><span class="comment">//用到的函数：rank、concat_ws、collect_set、split、unix_timestamp、子查询</span></span><br><span class="line"><span class="comment">//分析：</span></span><br><span class="line">首先要完成这种数据漏斗，必须能够将一个用户的每次访问严格的界定开来，即有办法区分每个用户的每次访问都请求了哪些网址，以及需要明确的特殊情况：</span><br><span class="line">Q：如果一个用户在访问下一个页面之前多次访问上一个页面应该如何处理间隔如A-&gt;A-&gt;B-&gt;C-&gt;D</span><br><span class="line">A：按照最后一次访问的时间为准</span><br><span class="line"><span class="comment">//表结构设计：</span></span><br><span class="line">在不影响结论的情况下下，应该让表结构尽可能简单。必须字段包括：</span><br><span class="line"><span class="number">1</span>. session_id 用于界定每个用户的一次访问，在一次访问中的多个请求该字段相同</span><br><span class="line"><span class="number">2</span>. url 请求链接地址</span><br><span class="line"><span class="number">3</span>. req_time 访问页面的时间</span><br><span class="line"><span class="comment">//数据集：</span></span><br><span class="line">例如页面无缺失、顺序、超过间隔、无重复页面的数据集：</span><br><span class="line">session_id  req_url     req_time</span><br><span class="line">s_01        a.html      <span class="number">2017</span>-<span class="number">03</span>-<span class="number">26</span> <span class="number">08</span>:<span class="number">00</span>:<span class="number">00</span></span><br><span class="line">s_01        b.html      <span class="number">2017</span>-<span class="number">03</span>-<span class="number">26</span> <span class="number">10</span>:<span class="number">01</span>:<span class="number">00</span></span><br><span class="line">s_01        c.html      <span class="number">2017</span>-<span class="number">03</span>-<span class="number">26</span> <span class="number">10</span>:<span class="number">03</span>:<span class="number">00</span></span><br><span class="line">s_01        d.html      <span class="number">2017</span>-<span class="number">03</span>-<span class="number">26</span> <span class="number">10</span>:<span class="number">04</span>:<span class="number">00</span></span><br><span class="line">页面无缺失，顺序，超过间隔，有重复页面</span><br><span class="line">session_id  req_url     req_time</span><br><span class="line">s_02        a.html      <span class="number">2017</span>-<span class="number">03</span>-<span class="number">26</span> <span class="number">08</span>:<span class="number">00</span>:<span class="number">00</span></span><br><span class="line">s_02        a.html      <span class="number">2017</span>-<span class="number">03</span>-<span class="number">26</span> <span class="number">09</span>:<span class="number">00</span>:<span class="number">00</span></span><br><span class="line">s_02        b.html      <span class="number">2017</span>-<span class="number">03</span>-<span class="number">26</span> <span class="number">11</span>:<span class="number">01</span>:<span class="number">00</span></span><br><span class="line">s_02        c.html      <span class="number">2017</span>-<span class="number">03</span>-<span class="number">26</span> <span class="number">11</span>:<span class="number">03</span>:<span class="number">00</span></span><br><span class="line">s_02        d.html      <span class="number">2017</span>-<span class="number">03</span>-<span class="number">26</span> <span class="number">11</span>:<span class="number">04</span>:<span class="number">00</span></span><br><span class="line">......</span><br><span class="line"><span class="comment">//建表、加载数据</span></span><br><span class="line">use test_db</span><br><span class="line"><span class="function">create table <span class="title">t_visitlog</span><span class="params">(session_id string,req_url string,req_time timestamp)</span></span></span><br><span class="line"><span class="function">row format delimited fields terminated by ' '</span>;</span><br><span class="line">hdfs dfs -put visitlog.data /xxx/xx/test_db.db/t_visitlog/</span><br><span class="line"><span class="comment">//去掉重复的页面记录</span></span><br><span class="line">create table t_vlog_norepeat as </span><br><span class="line">select session_id,req_url,req_time </span><br><span class="line">from(</span><br><span class="line">select session_id,req_url,req_time,rank() over(partition by session_id,req_url order by req_time desc) as rank </span><br><span class="line">from(</span><br><span class="line">select session_id,req_url,req_time from t_visitlog distribute by session_id,req_url sort by session_id,req_time desc</span><br><span class="line">	)a</span><br><span class="line">)b</span><br><span class="line">where b.rank = <span class="number">1</span>;</span><br><span class="line"><span class="comment">//合并访问记录</span></span><br><span class="line">create table t_vlog_merge as </span><br><span class="line">select session_id,concat_ws(<span class="string">','</span>,collect_set(vr)) as vtext </span><br><span class="line">from(</span><br><span class="line">select distinct session_id,concat_ws(<span class="string">'_'</span>,cast(req_time as string),req_url) as vr </span><br><span class="line">from(</span><br><span class="line">select distinct session_id,req_url,req_time from t_vlog_norepeat sort by session_id,req_time asc</span><br><span class="line">	)a</span><br><span class="line">) temp</span><br><span class="line">group by session_id;</span><br><span class="line"><span class="comment">//结果如下：</span></span><br><span class="line">s_01    <span class="number">2017</span>-<span class="number">03</span>-<span class="number">26</span> <span class="number">08</span>:<span class="number">00</span>:<span class="number">00</span>_a.html,<span class="number">2017</span>-<span class="number">03</span>-<span class="number">26</span> <span class="number">10</span>:<span class="number">01</span>:<span class="number">00</span>_b.html,<span class="number">2017</span>-<span class="number">03</span>-<span class="number">26</span> <span class="number">10</span>:<span class="number">03</span>:<span class="number">00</span>_c.html,<span class="number">2017</span>-<span class="number">03</span>-<span class="number">26</span> <span class="number">10</span>:<span class="number">04</span>:<span class="number">00</span>_d.html</span><br><span class="line">s_02    <span class="number">2017</span>-<span class="number">03</span>-<span class="number">26</span> <span class="number">09</span>:<span class="number">00</span>:<span class="number">00</span>_a.html,<span class="number">2017</span>-<span class="number">03</span>-<span class="number">26</span> <span class="number">11</span>:<span class="number">01</span>:<span class="number">00</span>_b.html,<span class="number">2017</span>-<span class="number">03</span>-<span class="number">26</span> <span class="number">11</span>:<span class="number">03</span>:<span class="number">00</span>_c.html,<span class="number">2017</span>-<span class="number">03</span>-<span class="number">26</span> <span class="number">11</span>:<span class="number">04</span>:<span class="number">00</span>_d.html</span><br><span class="line">s_03    <span class="number">2017</span>-<span class="number">03</span>-<span class="number">26</span> <span class="number">08</span>:<span class="number">00</span>:<span class="number">00</span>_a.html,<span class="number">2017</span>-<span class="number">03</span>-<span class="number">26</span> <span class="number">09</span>:<span class="number">01</span>:<span class="number">00</span>_b.html,<span class="number">2017</span>-<span class="number">03</span>-<span class="number">26</span> <span class="number">09</span>:<span class="number">03</span>:<span class="number">00</span>_c.html,<span class="number">2017</span>-<span class="number">03</span>-<span class="number">26</span> <span class="number">09</span>:<span class="number">04</span>:<span class="number">00</span>_d.html</span><br><span class="line">s_04    <span class="number">2017</span>-<span class="number">03</span>-<span class="number">26</span> <span class="number">09</span>:<span class="number">00</span>:<span class="number">00</span>_a.html,<span class="number">2017</span>-<span class="number">03</span>-<span class="number">26</span> <span class="number">10</span>:<span class="number">01</span>:<span class="number">00</span>_b.html,<span class="number">2017</span>-<span class="number">03</span>-<span class="number">26</span> <span class="number">10</span>:<span class="number">03</span>:<span class="number">00</span>_c.html,<span class="number">2017</span>-<span class="number">03</span>-<span class="number">26</span> <span class="number">10</span>:<span class="number">04</span>:<span class="number">00</span>_d.html</span><br><span class="line">s_05    <span class="number">2017</span>-<span class="number">03</span>-<span class="number">26</span> <span class="number">08</span>:<span class="number">00</span>:<span class="number">00</span>_a.html,<span class="number">2017</span>-<span class="number">03</span>-<span class="number">26</span> <span class="number">10</span>:<span class="number">01</span>:<span class="number">00</span>_b.html,<span class="number">2017</span>-<span class="number">03</span>-<span class="number">26</span> <span class="number">10</span>:<span class="number">04</span>:<span class="number">00</span>_d.html,<span class="number">2017</span>-<span class="number">03</span>-<span class="number">26</span> <span class="number">11</span>:<span class="number">04</span>:<span class="number">00</span>_c.html</span><br><span class="line">s_06    <span class="number">2017</span>-<span class="number">03</span>-<span class="number">26</span> <span class="number">09</span>:<span class="number">00</span>:<span class="number">00</span>_a.html,<span class="number">2017</span>-<span class="number">03</span>-<span class="number">26</span> <span class="number">10</span>:<span class="number">01</span>:<span class="number">00</span>_b.html,<span class="number">2017</span>-<span class="number">03</span>-<span class="number">26</span> <span class="number">10</span>:<span class="number">04</span>:<span class="number">00</span>_d.html,<span class="number">2017</span>-<span class="number">03</span>-<span class="number">26</span> <span class="number">11</span>:<span class="number">04</span>:<span class="number">00</span>_c.html</span><br><span class="line">s_07    <span class="number">2017</span>-<span class="number">03</span>-<span class="number">26</span> <span class="number">07</span>:<span class="number">00</span>:<span class="number">00</span>_a.html,<span class="number">2017</span>-<span class="number">03</span>-<span class="number">26</span> <span class="number">10</span>:<span class="number">01</span>:<span class="number">00</span>_b.html,<span class="number">2017</span>-<span class="number">03</span>-<span class="number">26</span> <span class="number">10</span>:<span class="number">04</span>:<span class="number">00</span>_d.html,<span class="number">2017</span>-<span class="number">03</span>-<span class="number">26</span> <span class="number">11</span>:<span class="number">04</span>:<span class="number">00</span>_c.html</span><br><span class="line">s_08    <span class="number">2017</span>-<span class="number">03</span>-<span class="number">26</span> <span class="number">07</span>:<span class="number">00</span>:<span class="number">00</span>_a.html,<span class="number">2017</span>-<span class="number">03</span>-<span class="number">26</span> <span class="number">10</span>:<span class="number">01</span>:<span class="number">00</span>_c.html,<span class="number">2017</span>-<span class="number">03</span>-<span class="number">26</span> <span class="number">10</span>:<span class="number">03</span>:<span class="number">00</span>_b.html,<span class="number">2017</span>-<span class="number">03</span>-<span class="number">26</span> <span class="number">10</span>:<span class="number">04</span>:<span class="number">00</span>_d.html</span><br><span class="line">s_09    <span class="number">2017</span>-<span class="number">03</span>-<span class="number">26</span> <span class="number">09</span>:<span class="number">00</span>:<span class="number">00</span>_a.html,<span class="number">2017</span>-<span class="number">03</span>-<span class="number">26</span> <span class="number">10</span>:<span class="number">01</span>:<span class="number">00</span>_b.html,<span class="number">2017</span>-<span class="number">03</span>-<span class="number">26</span> <span class="number">10</span>:<span class="number">04</span>:<span class="number">00</span>_d.html,<span class="number">2017</span>-<span class="number">03</span>-<span class="number">26</span> <span class="number">11</span>:<span class="number">04</span>:<span class="number">00</span>_c.html</span><br><span class="line">s_10    <span class="number">2017</span>-<span class="number">03</span>-<span class="number">26</span> <span class="number">09</span>:<span class="number">00</span>:<span class="number">00</span>_a.html,<span class="number">2017</span>-<span class="number">03</span>-<span class="number">26</span> <span class="number">10</span>:<span class="number">01</span>:<span class="number">00</span>_c.html,<span class="number">2017</span>-<span class="number">03</span>-<span class="number">26</span> <span class="number">10</span>:<span class="number">03</span>:<span class="number">00</span>_b.html,<span class="number">2017</span>-<span class="number">03</span>-<span class="number">26</span> <span class="number">10</span>:<span class="number">04</span>:<span class="number">00</span>_d.html</span><br><span class="line">s_11    <span class="number">2017</span>-<span class="number">03</span>-<span class="number">26</span> <span class="number">07</span>:<span class="number">00</span>:<span class="number">00</span>_a.html,<span class="number">2017</span>-<span class="number">03</span>-<span class="number">26</span> <span class="number">10</span>:<span class="number">03</span>:<span class="number">00</span>_b.html,<span class="number">2017</span>-<span class="number">03</span>-<span class="number">26</span> <span class="number">11</span>:<span class="number">04</span>:<span class="number">00</span>_d.html</span><br><span class="line">s_12    <span class="number">2017</span>-<span class="number">03</span>-<span class="number">26</span> <span class="number">07</span>:<span class="number">00</span>:<span class="number">00</span>_a.html,<span class="number">2017</span>-<span class="number">03</span>-<span class="number">26</span> <span class="number">10</span>:<span class="number">03</span>:<span class="number">00</span>_b.html,<span class="number">2017</span>-<span class="number">03</span>-<span class="number">26</span> <span class="number">10</span>:<span class="number">04</span>:<span class="number">00</span>_d.html</span><br><span class="line">s_13    <span class="number">2017</span>-<span class="number">03</span>-<span class="number">26</span> <span class="number">07</span>:<span class="number">00</span>:<span class="number">00</span>_a.html,<span class="number">2017</span>-<span class="number">03</span>-<span class="number">26</span> <span class="number">11</span>:<span class="number">04</span>:<span class="number">00</span>_d.html</span><br><span class="line">s_14    <span class="number">2017</span>-<span class="number">03</span>-<span class="number">26</span> <span class="number">07</span>:<span class="number">00</span>:<span class="number">00</span>_a.html,<span class="number">2017</span>-<span class="number">03</span>-<span class="number">26</span> <span class="number">10</span>:<span class="number">04</span>:<span class="number">00</span>_d.html</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h4 id="社区问答项目二："><a href="#社区问答项目二：" class="headerlink" title="社区问答项目二："></a>社区问答项目二：</h4><ol>
<li><p>关于报表以及标签存储选型：</p>
<p>MySQL行存储的方式比较适合OLTP业务。列存储的方式比较适合OLAP业务，而HBase采用了<strong>列族</strong>的方式平衡了OLTP和OLAP，<strong>支持水平扩展</strong> ，如果数据量比较大、对性能要求没有那么高、并且对事务没有要求的话，HBase也是个不错的考虑。ES默认对所有字段都建了索引，所以<strong>比较适合复杂的检索或全文检索</strong>。所以在本项目中将标签存入Hbase也是方便每天一列动态扩展加入Hbase，本身Hbase能够存储较大数据量，而Mysql关于增加列，需要提前alter表，比较麻烦效率低，之所以将最终标签放入ES是考虑两点，查询较快，在将标签体系提供给客户查询时能够很快检索出来，再就是ES对于大量的数据写入比较麻烦，需要对写入的字段再进行建立倒排索引，所以存在ES中的是最终标签。</p>
</li>
</ol>
<p>​    </p>
<p>[重分区]: </p>

      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/面试/" rel="tag"># 面试</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/05/03/面试题总结/" rel="next" title="面试题总结">
                <i class="fa fa-chevron-left"></i> 面试题总结
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>


  </div>


          </div>
          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">yimting@aliyun.com</p>
              <div class="site-description motion-element" itemprop="description">路漫漫其修远兮</div>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">13</span>
                    <span class="site-state-item-name">日志</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  
                    
                    
                      
                    
                    <span class="site-state-item-count">1</span>
                    <span class="site-state-item-name">分类</span>
                  
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  
                    
                      <a href="/tags/">
                    
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">9</span>
                    <span class="site-state-item-name">标签</span>
                  </a>
                </div>
              
            </nav>
          

          

          

          
            <div class="links-of-author motion-element">
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="mailto:yimting@aliyun.com" title="E-Mail &rarr; mailto:yimting@aliyun.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                </span>
              
            </div>
          

          

          
          

          
            
          
          

        </div>
      </div>

      
      <!--noindex-->
        <div class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
            
            
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#一面（2019-04-29）："><span class="nav-number">1.</span> <span class="nav-text">一面（2019.04.29）：</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#老陈二面（2019-05-06）："><span class="nav-number">2.</span> <span class="nav-text">老陈二面（2019.05.06）：</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#学长分享（2019-05-19）："><span class="nav-number">3.</span> <span class="nav-text">学长分享（2019.05.19）：</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#面试准备："><span class="nav-number">4.</span> <span class="nav-text">面试准备：</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#JVM调优"><span class="nav-number">4.0.1.</span> <span class="nav-text">JVM调优</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#什么是JVM？怎么理解JVM？JVM优化？"><span class="nav-number">4.0.2.</span> <span class="nav-text">什么是JVM？怎么理解JVM？JVM优化？</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Hive任务优化以及调优"><span class="nav-number">4.0.3.</span> <span class="nav-text">Hive任务优化以及调优</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Hive任务优化"><span class="nav-number">4.0.4.</span> <span class="nav-text">Hive任务优化</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#基本生产环境中的调优都是对小文件以及数据倾斜的处理"><span class="nav-number">4.0.5.</span> <span class="nav-text">基本生产环境中的调优都是对小文件以及数据倾斜的处理</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#hive数据倾斜"><span class="nav-number">4.0.6.</span> <span class="nav-text">hive数据倾斜</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#SparkShuffle"><span class="nav-number">4.0.7.</span> <span class="nav-text">SparkShuffle</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Kafka"><span class="nav-number">4.0.8.</span> <span class="nav-text">Kafka</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Spark调优"><span class="nav-number">4.0.9.</span> <span class="nav-text">Spark调优</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#增加并行度以及重分区："><span class="nav-number">4.0.10.</span> <span class="nav-text">增加并行度以及重分区：</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#flume拦截器作用："><span class="nav-number">4.0.11.</span> <span class="nav-text">flume拦截器作用：</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#flume断点续传"><span class="nav-number">4.0.12.</span> <span class="nav-text">flume断点续传</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#sqoop增量导入参数"><span class="nav-number">4.0.13.</span> <span class="nav-text">sqoop增量导入参数</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#缓慢变化维："><span class="nav-number">4.0.14.</span> <span class="nav-text">缓慢变化维：</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Hive函数："><span class="nav-number">4.0.15.</span> <span class="nav-text">Hive函数：</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#开窗函数"><span class="nav-number">4.0.15.1.</span> <span class="nav-text">开窗函数</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Mysql"><span class="nav-number">4.0.16.</span> <span class="nav-text">Mysql</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#常见shell命令"><span class="nav-number">4.0.17.</span> <span class="nav-text">常见shell命令</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#hive的几种启动方式："><span class="nav-number">4.0.18.</span> <span class="nav-text">hive的几种启动方式：</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#创建DataFrame的方式："><span class="nav-number">4.0.19.</span> <span class="nav-text">创建DataFrame的方式：</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#什么是Hadoop？"><span class="nav-number">4.0.20.</span> <span class="nav-text">什么是Hadoop？</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Spark基本构架以及原理"><span class="nav-number">4.0.21.</span> <span class="nav-text">Spark基本构架以及原理</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#kettle"><span class="nav-number">4.0.22.</span> <span class="nav-text">kettle</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#MR全流程："><span class="nav-number">4.0.23.</span> <span class="nav-text">MR全流程：</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Yarn工作机制"><span class="nav-number">4.0.24.</span> <span class="nav-text">Yarn工作机制</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#HA机制：C-Users-T-whong-Desktop-QS-FS-面试宝典76"><span class="nav-number">4.0.25.</span> <span class="nav-text">HA机制：C:\Users\T-whong\Desktop\QS\FS\面试宝典76</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#各种排序算法的使用："><span class="nav-number">4.0.26.</span> <span class="nav-text">各种排序算法的使用：</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Hive复杂数据类型"><span class="nav-number">4.0.27.</span> <span class="nav-text">Hive复杂数据类型</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Hive行列转换"><span class="nav-number">4.0.28.</span> <span class="nav-text">Hive行列转换</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#列转行："><span class="nav-number">4.0.28.1.</span> <span class="nav-text">列转行：</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#行转列："><span class="nav-number">4.0.28.2.</span> <span class="nav-text">行转列：</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Redis缓存雪崩缓存穿透"><span class="nav-number">4.0.29.</span> <span class="nav-text">Redis缓存雪崩缓存穿透</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#缓存穿透："><span class="nav-number">4.0.29.1.</span> <span class="nav-text">缓存穿透：</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#缓存雪崩（缓存失效）"><span class="nav-number">4.0.29.2.</span> <span class="nav-text">缓存雪崩（缓存失效）</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#热点key"><span class="nav-number">4.0.29.3.</span> <span class="nav-text">热点key</span></a></li></ol></li></ol></li></ol><li class="nav-item nav-level-3"><a class="nav-link" href="#简历附件："><span class="nav-number">5.</span> <span class="nav-text">简历附件：</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#蜂首项目一："><span class="nav-number">5.1.</span> <span class="nav-text">蜂首项目一：</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#项目总结："><span class="nav-number">5.1.1.</span> <span class="nav-text">项目总结：</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#第一个项目总结1："><span class="nav-number">5.1.2.</span> <span class="nav-text">第一个项目总结1：</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#第一个项目总结2："><span class="nav-number">5.1.3.</span> <span class="nav-text">第一个项目总结2：</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#第一个项目总结3："><span class="nav-number">5.1.4.</span> <span class="nav-text">第一个项目总结3：</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#责任描述1："><span class="nav-number">5.1.5.</span> <span class="nav-text">责任描述1：</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#责任描述2："><span class="nav-number">5.1.6.</span> <span class="nav-text">责任描述2：</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#责任描述3："><span class="nav-number">5.1.7.</span> <span class="nav-text">责任描述3：</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#社区问答项目二："><span class="nav-number">5.2.</span> <span class="nav-text">社区问答项目二：</span></a></li></ol></li></div>
            

          </div>
        </div>
      <!--/noindex-->
      

      

    </div>
  </aside>
  


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">yimting@aliyun.com</span>

  

  
</div>


  <div class="powered-by">由 <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v3.8.0</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> v7.1.0</div>




        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  <script src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>


  


  <script src="/js/utils.js?v=7.1.0"></script>

  <script src="/js/motion.js?v=7.1.0"></script>



  
  


  <script src="/js/schemes/muse.js?v=7.1.0"></script>



  
  <script src="/js/scrollspy.js?v=7.1.0"></script>
<script src="/js/post-details.js?v=7.1.0"></script>



  


  <script src="/js/next-boot.js?v=7.1.0"></script>


  

  

  

  


  


  




  

  

  

  

  

  
  <script>
    (function(){
      var bp = document.createElement('script');
      var curProtocol = window.location.protocol.split(':')[0];
      bp.src = (curProtocol === 'https') ? 'https://zz.bdstatic.com/linksubmit/push.js' : 'http://push.zhanzhang.baidu.com/push.js';
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(bp, s);
    })();
  </script>


  

  

  

  

  

  

  

  

</body>
</html>
